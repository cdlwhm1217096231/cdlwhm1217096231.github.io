{"meta":{"title":"Curry_Coder的空间","subtitle":"好好学习，天天向上！","description":"怕什么真理无穷，进一寸有一寸的欢喜。","author":"Curry_Coder","url":"https://cdlwhm1217096231.github.io","root":"/"},"pages":[{"title":"","date":"2019-07-29T06:19:58.659Z","updated":"2019-07-29T06:19:58.659Z","comments":true,"path":"404/index.html","permalink":"https://cdlwhm1217096231.github.io/404/index.html","excerpt":"","text":"&lt;!DOCTYPE HTML&gt;"},{"title":"categories","date":"2019-07-29T02:01:52.000Z","updated":"2019-07-29T02:09:41.427Z","comments":true,"path":"categories/index.html","permalink":"https://cdlwhm1217096231.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-07-29T02:01:45.000Z","updated":"2019-07-29T02:10:02.356Z","comments":true,"path":"tags/index.html","permalink":"https://cdlwhm1217096231.github.io/tags/index.html","excerpt":"","text":""},{"title":"about","date":"2019-07-29T02:01:42.000Z","updated":"2019-07-29T10:32:45.107Z","comments":true,"path":"about/index.html","permalink":"https://cdlwhm1217096231.github.io/about/index.html","excerpt":"","text":"关于我 普通二本院校毕业，现就读于某院校读研三，研究方向为机器学习、NLP中的文本分类与机器翻译，欢迎与您进行更多技术交流。"},{"title":"archives","date":"2019-07-29T02:07:43.000Z","updated":"2019-07-29T02:09:10.555Z","comments":true,"path":"archives/index.html","permalink":"https://cdlwhm1217096231.github.io/archives/index.html","excerpt":"","text":""}],"posts":[{"title":"神经网络中的批量归一化Batch Normalization(BN)原理总结","slug":"神经网络中的批量归一化Batch-Normalization-BN-原理总结","date":"2019-07-29T11:02:46.000Z","updated":"2019-07-29T11:05:18.738Z","comments":true,"path":"学习/神经网络中的批量归一化Batch-Normalization-BN-原理总结/","link":"","permalink":"https://cdlwhm1217096231.github.io/学习/神经网络中的批量归一化Batch-Normalization-BN-原理总结/","excerpt":"","text":"0.概述 深层神经网络存在的问题(从当前层的输入的分布来分析)：在深层神经网络中，中间层的输入是上一层神经网络的输出。因此，之前的层的神经网络参数的变化会导致当前层输入的分布发生较大的差异。在使用随机梯度下降法来训练神经网络时，每次参数更新都会导致网络中每层的输入分布发生变化。越是深层的神经网络，其输入的分布会改变的越明显。 解决方法(归一化操作)：从机器学习角度来看，如果某层的输入分布发生了变化，那么其参数需要重新学习，这种现象称为内部协变量偏移。为了解决内部协变量偏移问题，就要使得每一层神经网络输入的分布在训练过程中要保持一致。最简单的方法是对每一层神经网络都进行归一化操作，使其分布保持稳定。 1.批量归一化 协变量偏移介绍 在传统机器学习中，一个常见的问题是协变量偏移。协变量是一个统计学概念，是可能影响预测结果的统计变量。在机器学习中，协变量可以看作是输入。一般的机器学习算法都要求输入在训练集和测试集上的分布式相似的。如果不满足这个假设，在训练集上学习到的模型在测试集上的表现会比较差，如下图所示： BN原理介绍 批量归一化方法是一种有效的逐层归一化的方法，可以对神经网络中任意的中间层进行归一化操作。对一个深层神经网络来说，令第l层的净输入为$\\mathbf{z}^{(l)}$， 经过激活函数后的输出是$\\mathbf{a}^{(l)}$，即 \\mathbf{a}^{(l)}=f\\left(\\mathbf{z}^{(l)}\\right)=f\\left(W \\mathbf{a}^{(l-1)}+\\mathbf{b}\\right)其中，$f(\\cdot)$是激活函数，W和b是权重和偏置参数。 为了减少内部协变量偏移问题，就要使得净输入$\\mathbf{z}^{(l)}$的分布一致，比如都归一化到标准正态分布。虽然归一化操作可以应用在输入$\\mathbf{a}^{(l-1)}$上，但其分布性质不如$\\mathbf{z}^{(l)}$稳定。因此，在实践中归一化操作一般应用在仿射变换之后，在下一次激活函数之前。利用数据预处理方法对$\\mathbf{z}^{(l)}$进行归一化，相当于每一层都进行一次数据预处理，从而加速收敛速度。但是，逐层归一化需要在中间层进行操作，要求效率比较高，因此复杂度比较高的白话方法就不太合适。为了提高归一化效率，一般使用标准归一化，将净输入$\\mathbf{z}^{(l)}$的每一维都归一到标准正态分布。 \\hat{\\mathbf{z}}^{(l)}=\\frac{\\mathbf{z}^{(l)}-\\mathbb{E}\\left[\\mathbf{z}^{(l)}\\right]}{\\sqrt{\\operatorname{var}\\left(\\mathbf{z}^{(l)}\\right)+\\epsilon}}其中，$\\mathbb{E}\\left[\\mathbf{z}^{(l)}\\right]$和$\\operatorname{var}\\left(\\mathbf{z}^{(l)}\\right)$是当前参数下，$\\mathbf{z}^{(l)}$的每一维度在整个训练集上的期望和方差。因为目前主要的训练方法是基于Mini-Batch的随机梯度下降算法，所以准确地计算$\\mathbf{z}^{(l)}$的期望和方差是不可行的。因此，$\\mathbf{z}^{(l)}$的期望和方差通常用当前小批量Mini-Batch样本集的均值和方差近似估计。 给定一个包含K个样本的小批量样本集合，第l层神经元的净输入$\\mathbf{z}^{(1,l)}$，….，$\\mathbf{z}^{(K,l)}$的均值和方差为： \\begin{aligned} \\mu_{\\mathcal{B}} &=\\frac{1}{K} \\sum_{k=1}^{\\mathrm{N}} \\mathbf{z}^{(k, l)} \\\\ \\sigma_{\\mathcal{B}}^{2} &=\\frac{1}{K} \\sum_{k=1}^{K}\\left(\\mathbf{z}^{(k, l)}-\\mu_{\\mathcal{B}}\\right) \\odot\\left(\\mathbf{z}^{(k, l)}-\\mu_{\\mathcal{B}}\\right) \\end{aligned} 对净输入$\\mathbf{z}^{(l)}$的标准归一化会使得其取值集中到0附近，如果使用sigmoid激活函数时，这个取值区间刚好是接近线性变换区间，从而减弱了神经网络非线性变换的性质。因此，为了使归一化操作不对网络的表示能力造成负面影响，可以通过一个附加的缩放和平移变换改变取值区间。$\\hat{\\mathbf{z}}^{(l)}=\\frac{\\mathbf{z}^{(l)}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\odot \\gamma+\\beta\\triangleq \\mathrm{BN}_{\\gamma, \\beta}\\left(\\mathbf{z}^{(l)}\\right)$其中，$\\gamma$、$\\beta$分别表示缩放和平移的参数向量。从最保守的角度考虑，可以通过标准归一化的逆变换来使得归一化的变量可以被还原为原来的值。即：当$\\gamma=\\sqrt{\\sigma_{\\mathcal{B}}^{2}}$，$\\beta=\\mu_{\\mathcal{B}}$时，$\\hat{\\mathbf{z}}^{(l)}=\\mathbf{z}^{(l)}$。 批量归一化操作可以看作是一个特殊的神经网络层，该层是加在每一层非线性激活函数之前，即： \\mathbf{a}^{(l)}=f\\left(\\mathbf{B} \\mathbf{N}_{\\gamma, \\beta}\\left(\\mathbf{z}^{(l)}\\right)\\right)=f\\left(\\mathbf{B} \\mathbf{N}_{\\gamma, \\beta}\\left(W \\mathbf{a}^{(l-1)}\\right)\\right)其中，因为批量归一化本身具有平移变换，因此非线性变换$W \\mathbf{a}^{(l-1)}$就不再需要偏置参数b。 注意：每次小批量样本的$\\mu_{\\mathcal{B}}$和$\\sigma_{\\mathcal{B}}^{2}$是净输入$\\mathbf{z}^{(l)}$的函数，而不是常量。因此，在计算参数梯度时，需要考虑$\\mu_{\\mathcal{B}}$和$\\sigma_{\\mathcal{B}}^{2}$的影响。当训练完成时，用整个数据集上的均值$\\mu$和方差$\\sigma^{2}$来分别替代每次小批量样本的$\\mu_{\\mathcal{B}}$和$\\sigma_{\\mathcal{B}}^{2}$。在实际中，$\\mu_{\\mathcal{B}}$和$\\sigma_{\\mathcal{B}}^{2}$也可以使用移动平均来计算。 2.参考资料 邱锡鹏：《神经网络与深度学习》","categories":[{"name":"学习","slug":"学习","permalink":"https://cdlwhm1217096231.github.io/categories/学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://cdlwhm1217096231.github.io/tags/人工智能/"},{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://cdlwhm1217096231.github.io/tags/算法/"},{"name":"深度学习","slug":"深度学习","permalink":"https://cdlwhm1217096231.github.io/tags/深度学习/"}]},{"title":"常见的数据预处理方法总结","slug":"常见的数据预处理方法总结","date":"2019-07-29T10:50:47.000Z","updated":"2019-07-29T11:00:15.136Z","comments":true,"path":"学习/常见的数据预处理方法总结/","link":"","permalink":"https://cdlwhm1217096231.github.io/学习/常见的数据预处理方法总结/","excerpt":"","text":"0.概述 一般而言，样本的原始特征中的每一维特征由于来源以及度量单位不同，其特征取值的分布范围往往差异很大。当我们计算不同样本之间的欧氏距离时，取值范围大的特征会起到主导作用。这样，对于基于相似度比较的机器学习方法（比如最近邻分类器KNN），必须先对样本进行预处理，将各个维度的特征归一化到同一个取值区间，并且消除不同特征之间的相关性，才能获得比较理想的结果。虽然神经网络可以通过参数的调整来适应不同特征的取值范围，但是会导致训练效率比较低。 假设一个只有一层的网络 $y=\\tanh \\left(w_{1} x_{1}+w_{2} x_{2}+b\\right)$，其中$x_{1} \\in[0,10]$，$x_{2} \\in[0,1]$。因为tanh函数的导数在区间 [−2, 2]上是敏感的，其余地方的导数接近于 0。因此，如果 $w_{1} x_{1}+w_{2} x_{2}+b$过大或过小，都会导致梯度过小，难以训练。为了提高训练效率，我们需要使$w_{1} x_{1}+w_{2} x_{2}+b$在 [−2, 2]区间，我们需要将w1设得小一点，比如在 [−0.1,0.1]之间。可以想象，如果数据维数很多时，我们很难这样精心去选择每一个参数。因此，如果每一个特征的取值范围都在相似的区间，比如 [0, 1]或者 [−1, 1]，我们就不太需要区别对待每一个参数，减少人工干预。 当不同输入特征的取值范围差异比较大时，梯度下降法的效率也会受到影响。下图给出了数据归一化对梯度的影响。其中，图a为未归一化数据的等高线图。取值范围不同会造成在大多数位置上的梯度方向并不是最优的搜索方向。当使用梯度下降法寻求最优解时，会导致需要很多次迭代才能收敛。如果我们把数据归一化为取值范围相同，如图b所示，大部分位置的梯度方向近似于最优搜索方向。这样，在梯度下降求解时，每一步梯度的方向都基本指向最小值，训练效率会大大提高。 1.常用的归一化方法 1.1 缩放归一化：通过缩放将每一个特征的取值范围归一到 [0, 1]或 [−1, 1]之间。假设有 N 个样本$\\left\\{\\mathbf{x}^{(n)}\\right\\}_{n=1}^{N}$，对每一维特征x， \\hat{x}^{(n)}=\\frac{x^{(n)}-\\min _{n}\\left(x^{(n)}\\right)}{\\max _{n}\\left(x^{(n)}\\right)-\\min _{n}\\left(x^{(n)}\\right)}其中，min(x)和max(x)分别是特征x在所有样本上的最小值和最大值。 1.2 标准归一化：将每一个维特征都处理为符合标准正态分布（均值为 0，标准差为 1）。假设有 N 个样本$\\left\\{\\mathbf{x}^{(n)}\\right\\}_{n=1}^{N}$，对每一维特征x，先计算它的均值和标准差： \\begin{aligned} \\mu &=\\frac{1}{N} \\sum_{n=1}^{N} x^{(n)} \\\\ \\sigma^{2} &=\\frac{1}{N} \\sum_{n=1}^{N}\\left(x^{(n)}-\\mu\\right)^{2} \\end{aligned}然后，将特征$x^{(n)}$减去均值，并除以标准差，得到新的特征$\\hat{x}^{(n)}$。 \\hat{x}^{(n)}=\\frac{x^{(n)}-\\mu}{\\sigma}这里$\\sigma$不能为0，如果标准差为0，则说明这一维度的特征没有任务的区分性，可以直接删除。在标准归一化之后，每一维特征都服从标准正态分布。 1.3 白化：是一种重要的预处理方法，用来降低输入数据特征之间的冗余性。输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差。白化的一个主要实现方式是使用主成分分析PCA方法去除掉各个成分之间的相关性。 2. 参考资料 邱锡鹏：《神经网络与深度学习》","categories":[{"name":"学习","slug":"学习","permalink":"https://cdlwhm1217096231.github.io/categories/学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://cdlwhm1217096231.github.io/tags/人工智能/"},{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://cdlwhm1217096231.github.io/tags/算法/"}]},{"title":"复习02统计学习方法(感知机perceptron machine)---图片版","slug":"复习02统计学习方法-感知机perceptron-machine-图片版","date":"2019-07-29T10:48:41.000Z","updated":"2019-07-29T10:49:50.474Z","comments":true,"path":"学习/复习02统计学习方法-感知机perceptron-machine-图片版/","link":"","permalink":"https://cdlwhm1217096231.github.io/学习/复习02统计学习方法-感知机perceptron-machine-图片版/","excerpt":"","text":"","categories":[{"name":"学习","slug":"学习","permalink":"https://cdlwhm1217096231.github.io/categories/学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://cdlwhm1217096231.github.io/tags/人工智能/"},{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://cdlwhm1217096231.github.io/tags/算法/"}]},{"title":"复习01统计学习方法概论(机器学习中的重要概念)---图片版","slug":"复习01统计学习方法概论-机器学习中的重要概念-图片版","date":"2019-07-29T10:46:17.000Z","updated":"2019-07-29T10:47:28.649Z","comments":true,"path":"学习/复习01统计学习方法概论-机器学习中的重要概念-图片版/","link":"","permalink":"https://cdlwhm1217096231.github.io/学习/复习01统计学习方法概论-机器学习中的重要概念-图片版/","excerpt":"","text":"","categories":[{"name":"学习","slug":"学习","permalink":"https://cdlwhm1217096231.github.io/categories/学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://cdlwhm1217096231.github.io/tags/人工智能/"},{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://cdlwhm1217096231.github.io/tags/算法/"}]},{"title":"数据库中的基本概念","slug":"数据库中的基本概念","date":"2019-07-28T06:50:37.000Z","updated":"2019-07-28T04:05:54.000Z","comments":true,"path":"数据库/数据库中的基本概念/","link":"","permalink":"https://cdlwhm1217096231.github.io/数据库/数据库中的基本概念/","excerpt":"","text":"1.基本概念 数据（data）：描述事物的符号记录称为数据 数据库（DataBase，DB）：是长期存储在计算机内、有组织的、可共享的大量数据的集合，具有永久存储、有组织、可共享三个基本特点。 数据库管理系统（DataBase Management System，DBMS）：是位于用户与操作系统之间的一层数据管理软件。 数据库系统（DataBase System，DBS）：是有数据库、数据库管理系统（及其应用开发工具）、应用程序和数据库管理员（DataBase Administrator DBA）组成的存储、管理、处理和维护数据的系统。 实体（entity）：客观存在并可相互区别的事物称为实体。 属性（attribute）：实体所具有的某一特性称为属性。 码（key）：唯一标识实体的属性集称为码。 实体型（entity type）：用实体名及其属性名集合来抽象和刻画同类实体，称为实体型。 实体集（entity set）：同一实体型的集合称为实体集。 联系（relationship）：实体之间的联系通常是指不同实体集之间的联系。 模式（schema）：模式也称逻辑模式，是数据库全体数据的逻辑结构和特征的描述，是所有用户的公共数据视图。 外模式（external schema）：外模式也称子模式（subschema）或用户模式，它是数据库用户（包括应用程序员和最终用户）能够看见和使用的局部数据的逻辑结构和特征的描述，是数据库用户的数据视图，是与某一应用有关的数据的逻辑表示。 内模式（internal schema）：内模式也称为存储模式（storage schema），一个数据库只有一个内模式。他是数据物理结构和存储方式的描述，是数据库在数据库内部的组织方式。2.常用数据模型 层次模型（hierarchical model） 网状模型（network model） 关系模型（relational model） 关系（relation）：一个关系对应通常说的一张表 元组（tuple）：表中的一行即为一个元组 属性（attribute）：表中的一列即为一个属性 码（key）：表中可以唯一确定一个元组的某个属性组 域（domain）：一组具有相同数据类型的值的集合 分量：元组中的一个属性值 关系模式：对关系的描述，一般表示为 关系名(属性1, 属性2, …, 属性n) 面向对象数据模型（object oriented data model） 对象关系数据模型（object relational data model） 半结构化数据模型（semistructure data model）3.常用SQL操作 更多操作请参见我之前总结的数据库基本操作4.关系型数据库 基本关系操作：查询（选择、投影、连接（等值连接、自然连接、外连接（左外连接、右外连接））、除、并、差、交、笛卡尔积等）、插入、删除、修改 关系模型中的三类完整性约束：实体完整性、参照完整性、用户定义的完整性5.索引 数据库索引：顺序索引、B+ 树索引、hash 索引 更多信息: MySQL 索引背后的数据结构及算法原理6.数据库完整性 数据库的完整性是指数据的正确性和相容性。 完整性：为了防止数据库中存在不符合语义（不正确）的数据。 安全性：为了保护数据库防止恶意破坏和非法存取。 触发器：是用户定义在关系表中的一类由事件驱动的特殊过程。7.关系数据理论 数据依赖是一个关系内部属性与属性之间的一种约束关系，是通过属性间值的相等与否体现出来的数据间相关联系。 最重要的数据依赖：函数依赖、多值依赖。8.范式 第一范式（1NF）：属性（字段）是最小单位不可再分。 第二范式（2NF）：满足 1NF，每个非主属性完全依赖于主键（消除 1NF 非主属性对码的部分函数依赖）。 第三范式（3NF）：满足 2NF，任何非主属性不依赖于其他非主属性（消除 2NF 主属性对码的传递函数依赖）。 鲍依斯-科得范式（BCNF）：满足 3NF，任何非主属性不能对主键子集依赖（消除 3NF 主属性对码的部分和传递函数依赖）。 第四范式（4NF）：满足 3NF，属性之间不能有非平凡且非函数依赖的多值依赖（消除 3NF 非平凡且非函数依赖的多值依赖）。9.数据库恢复 事务：是用户定义的一个数据库操作序列，这些操作要么全做，要么全不做，是一个不可分割的工作单位。 事物的 ACID 特性：原子性、一致性、隔离性、持续性。 恢复的实现技术：建立冗余数据 -&gt; 利用冗余数据实施数据库恢复。 建立冗余数据常用技术：数据转储（动态海量转储、动态增量转储、静态海量转储、静态增量转储）、登记日志文件。10.并发控制 事务是并发控制的基本单位。 并发操作带来的数据不一致性包括：丢失修改、不可重复读、读 “脏” 数据。 并发控制主要技术：封锁、时间戳、乐观控制法、多版本并发控制等。 基本封锁类型：排他锁（X 锁 / 写锁）、共享锁（S 锁 / 读锁）。 活锁和死锁： 活锁：事务永远处于等待状态，可通过先来先服务的策略避免。 死锁：事物永远不能结束 预防：一次封锁法、顺序封锁法； 诊断：超时法、等待图法； 解除：撤销处理死锁代价最小的事务，并释放此事务的所有的锁，使其他事务得以继续运行下去。 可串行化调度：多个事务的并发执行是正确的，当且仅当其结果与按某一次序串行地执行这些事务时的结果相同。可串行性时并发事务正确调度的准则。参考博客 数据库简明教程","categories":[{"name":"数据库","slug":"数据库","permalink":"https://cdlwhm1217096231.github.io/categories/数据库/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://cdlwhm1217096231.github.io/tags/数据库/"}]}]}
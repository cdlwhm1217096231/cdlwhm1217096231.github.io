{"meta":{"title":"Curry_Coder的空间","subtitle":"好好学习，天天向上！","description":"怕什么真理无穷，进一寸有一寸的欢喜。","author":"Curry_Coder","url":"https://cdlwhm1217096231.github.io","root":"/"},"pages":[{"title":"about","date":"2019-07-29T02:01:42.000Z","updated":"2019-07-29T10:32:45.107Z","comments":true,"path":"about/index.html","permalink":"https://cdlwhm1217096231.github.io/about/index.html","excerpt":"","text":"关于我 普通二本院校毕业，现就读于某院校读研三，研究方向为机器学习、NLP中的文本分类与机器翻译，欢迎与您进行更多技术交流。"},{"title":"tags","date":"2019-07-29T02:01:45.000Z","updated":"2019-07-29T02:10:02.356Z","comments":true,"path":"tags/index.html","permalink":"https://cdlwhm1217096231.github.io/tags/index.html","excerpt":"","text":""},{"title":"archives","date":"2019-07-29T02:07:43.000Z","updated":"2019-07-29T02:09:10.555Z","comments":true,"path":"archives/index.html","permalink":"https://cdlwhm1217096231.github.io/archives/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-07-29T02:01:52.000Z","updated":"2019-07-29T02:09:41.427Z","comments":true,"path":"categories/index.html","permalink":"https://cdlwhm1217096231.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2019-07-29T06:19:58.659Z","updated":"2019-07-29T06:19:58.659Z","comments":true,"path":"404/index.html","permalink":"https://cdlwhm1217096231.github.io/404/index.html","excerpt":"","text":"&lt;!DOCTYPE HTML&gt; L2Dwidget.init({\"pluginRootPath\":\"live2dw/\",\"pluginJsPath\":\"lib/\",\"pluginModelPath\":\"assets/\",\"model\":{\"jsonPath\":\"/live2dw/assets/hijiki.model.json\"},\"display\":{\"position\":\"right\",\"width\":150,\"height\":300},\"mobile\":{\"show\":true},\"log\":false,\"tagMode\":false});"}],"posts":[{"title":"Linux中的文件打包与压缩方法总结","slug":"Linux中的文件打包与压缩方法总结","date":"2019-07-29T11:45:34.000Z","updated":"2019-07-29T11:55:17.728Z","comments":true,"path":"Linux/Linux中的文件打包与压缩方法总结/","link":"","permalink":"https://cdlwhm1217096231.github.io/Linux/Linux中的文件打包与压缩方法总结/","excerpt":"","text":"一、文件打包和解压缩 在windows系统上最常见的压缩文件不外乎这三种*.zip,*.rar,*.7z后缀的压缩文件，而在Linux系统上常见常用的除了以上的三种之外，还有*.gz,*.xz,*.bz2,*.tar,*tar.gz,*tar.xz,*.tar.bz2等后缀的压缩文件。 1..zip压缩打包程序 1.1 打包文件夹zip -r -q -o njust.zip /home/cdl 参数说明: -r：表示递归打包包含子目录的全部内容 -q：表示为安静模式，即不向屏幕输出信息 -o：表示输出文件，需要在其后紧跟打包输出文件名 查看压缩文件的信息： 12du -h njust.zipfile njust.zip 1.2 设置压缩等级(9最小,1最大) zip -r 9 -q -o njust_9.zip /home/cdl -x ~/*.zip zip -r -1 -q -o njust_1.zip /home/cdl -x ~/*.zip这里添加了一个参数用于设置压缩级别[1-9],1表示最快压缩但体积大，9表示体积最小但耗时最久。最后那个-x是为了排除上一次创建的zip文件，否则又会被打包进这一次的压缩文件中。这里只能使用绝对路径！！！ 再使用du命令分别查看默认压缩等级、最低和最高压缩级别及未压缩的文件的大小： 1du -h -d 0 *.zip | sort 通过man手册可知： -h：输入人类可以解释的信息 -d：所查看文件的深度 1.3 创建加密zip包(使用-e参数可以创建加密压缩包) zip -r -e -o njust_encryption.zip /home/cdl 注意：关于zip命令，因为windows系统与Linux在文本文件格式上的兼容问题，比如换行符(为不可见字符)，在windows为回车加换行，Linux上为换行；所以如果不加处理的话，在Linux上编辑的文本文件，在windows系统上打开可能看起来是没有换行的。如果想让在Linux创建的zip压缩文件在Windows系统上解压后没有任何问题，那么还需要对命令进行修改： zip -r -l -o njust.zip /home/cdl 需要加上-l参数将换行转为回车加换行 2.使用unzip命令解压缩zip文件 将njust.zip解压到当前目录下： unzip njust.zip 使用安静模式，将文件解压到指定目录，指定目录不存在，会自动创建： unzip -q njust.zip -d ziptest 如果不想解压只想查看压缩包的内容，可以使用-l参数： unzip -l njust.zip 注意：使用unzip解压文件时同样应该注意兼容问题，不过这里此时关心的不再是上面的问题，而是中文编码的问题。通常windows系统上创建的压缩文件，如果有包含中文的文档或以中文作为文件名的文件时，默认会采用GBK或其他编码，而Linux上默认使用utf-8编码，如果不加任何处理，直接解压的话可能会出现中文乱码的问题(有时候它会帮你自动处理)。为了解决这个问题，可以在解压时指定编码类型。 使用-O(大写的字母O)参数指定编码类型： 1unzip -O GBK 中文压缩文件.zip 3..rar文件打包压缩命令 rar也是windows上常用的一种压缩文件的格式，在Linux上可以使用rar和unrar工具分别创建和解压rar压缩包 首先使用需要安装rar和unrar工具： sudo apt-get update sudo apt-get install rar unrar 在使用rar、unrar命令时，应该注意命令参数前不加-，否则会报错！ rm *.zip rar a njust.rar上面的命令使用a参数添加一个目录~到一个归档文件中，如果该文件不存在就会自动创建。 如果不解压只是查看文件，可以使用参数l： rar l njust.rar 全路径解压： unrar x njust.rar 去掉全路径解压： mkdir temp unrar e njust.rar temp/ 4..tar打包工具 在Linux上面更常用的是tar工具，tar原本只是一个打包工具，只是同时还实现了对7z，gizp，xz，bzip2等工具的支持 创建一个tar包：tar -cf njust.tar ~ (此命令会自动去掉表示绝对路径的/，也可以使用-P保留绝对路径符) 参数说明： -c：创建一个tar包文件 -f：指定创建文件的名，注意文件名必须紧跟在-f参数后，不能写成tar -fc njust.tar!可以写成tar -f njust.tar -c ~ -v：以可视的方式输出打包的文件 解压一个文件(-x参数)到指定路径的已存在目录(-C参数)：mkdir tardirtar -xf njust.tar -C tardir 只查看不解压文件-t参数：tar -tf njust.tar 对于创建不同压缩格式的文件时，对于tar来说是非常简单，需要的只是换一个参数，这里以使用gzip工具创建.tar.gz文件为例来说明。只需要在创建tar文件的基础上加一个-z参数，使用gzip来压缩文件：tar -czf njust.tar.gz ~ 解压*.tar.gz文件到当前文件夹：tar -xzf njust.tar.gz 现在要使用其他的压缩工具创建或解压相应文件时，只需要更改一个参数即可： 此外，还有gzip和gunzip(相当于gzip -d)：压缩和解压命令，解压文件为.gz后缀","categories":[{"name":"Linux","slug":"Linux","permalink":"https://cdlwhm1217096231.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://cdlwhm1217096231.github.io/tags/Linux/"}]},{"title":"TensorFlow中的RNNCell基本单元使用","slug":"TensorFlow中的RNNCell基本单元使用","date":"2019-07-29T11:43:57.000Z","updated":"2019-07-29T11:44:51.378Z","comments":true,"path":"深度学习/TensorFlow中的RNNCell基本单元使用/","link":"","permalink":"https://cdlwhm1217096231.github.io/深度学习/TensorFlow中的RNNCell基本单元使用/","excerpt":"","text":"0.charRNN基础介绍 charRNN 是N vs N的循环神经网络，要求输入序列长度等于输出序列长度。原理：用已经输入的字母去预测下一个字母的概率。一个句子是hello!,例如输入序列是hello,则输出序列是ello!预测时：首先选择一个x1当作起始的字符，然后用训练好的模型得到下一个字符出现的概率。根据这个概率选择一个字符输出，然后将此字符当作下一步的x2输入到模型中。依次递推，得到任意长度的文字。注意：输入的单个字母是以one-hot形式进行编码的！ 对中文进行建模时，每一步输入模型的是一个汉字，由于汉字的种类太多，导致模型太大，一般采用下面的方法进行优化： 1.取最常用的N个汉字，将剩下的汉字变成单独的一类，用一个\\字符来进行标注 2.在输入时，可以加入一个embedding层，将汉字的one-hot编码转为稠密的词嵌入表示。对单个字母不使用embedding是由于单个字母不具备任何的含义，只需要使用one-hot编码即可。单个汉字是具有一定的实际意义的，所以使用embedding层 1.实现RNN的基本单元RNNCell抽象类————有两种直接使用的子类:BasicRNNCell(基本的RNN)和LSTMCell(基本的LSTM) RNNCell有三个属性: 1.类方法call:所有的子类都会实现一个call函数，可以实现RNN的单步计算，调用形式：(output,nextstate)=\\_call__(input, state) 2.类属性statesize:隐藏层的大小，输入数据是以batch_size的形式进行输入的即input=(batch_size, input_size),调用\\_call__函数时隐藏层的形状是(batch_size, state_size),输出层的形状是(batch_size, output_size) 3.类属性output_size:输出向量的大小 2.定义一个基本的RNN单元12345import tensorflow as tfimport numpy as nprnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128)print(\"rnn_cell.state_size:\", rnn_cell.state_size) 3.定义一个基本的LSTM的基本单元123456789lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=128)print(\"lstm_cell.state_size:\", lstm_cell.state_size)lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=128) # batch_size=32, input_size=100inputs = tf.placeholder(np.float32, shape=(32, 100))h0 = lstm_cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态output, h1 = lstm_cell.__call__(inputs, h0)print(h1.c)print(h1.h) 4.对RNN进行堆叠：MultiRNNCell1234567891011121314# 每次调用这个函数返回一个BasicRNNCelldef get_a_cell(): return tf.nn.rnn_cell.BasicRNNCell(num_units=128)# 使用MultiRNNCell创建3层RNNcell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() for _ in range(3)])# 得到的RNN也是RNNCell的子类,state_size=(128, 128, 128):三个隐层状态，每个隐层状态的大小是128print(cell.state_size)# 32是batch_size, 100是input_sizeinputs = tf.placeholder(np.float32, shape=(32, 100))h0 = cell.zero_state(32, np.float32)output, h1 = cell.__call__(inputs, h0)print(h1) 5.使用tf.nn.dunamic_rnn按时间展开：相当于增加了一个时间维度time_steps,通过{h0,x1,x2…,xn}得到{h1,h2,h3,…hn}12345inputs: shape=(batch_size, time_steps, input_size) # 输入数据的格式是(batch_size, time_steps, input_size)initial_state: shape(batch_size,cell.state_size) # 初始状态,一般可以取零矩阵outputs, state = tf.nn.dynamic_rnn(cell,inputs,initial_state)# outputs是time_steps中所有的输出，形状是(batch_size, time_steps, cell.output_size)# state是最后一步的隐状态，形状是(batch_size,cell.state_size) 注意：输入数据的形状是(time_steps,batch_size, input_size),可以调用tf.nn.dynamic_rnn()函数中设定参数time_major=True。此时，得到的outputs的形状是(time_steps, batch_size, cell.output_size);state的形状不变化","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://cdlwhm1217096231.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://cdlwhm1217096231.github.io/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://cdlwhm1217096231.github.io/tags/TensorFlow/"}]},{"title":"Pandas中iloc、loc、ix三者的区别","slug":"Pandas中iloc、loc、ix三者的区别","date":"2019-07-29T11:40:56.000Z","updated":"2019-07-29T12:54:06.136Z","comments":true,"path":"数据分析/Pandas中iloc、loc、ix三者的区别/","link":"","permalink":"https://cdlwhm1217096231.github.io/数据分析/Pandas中iloc、loc、ix三者的区别/","excerpt":"","text":"一、综述：iloc、loc、ix可以用来索引数据、抽取数据二、iloc、loc、ix三者对比 iloc和loc的区别 iloc主要使用数字来索引数据，不能使用字符型的标签来索引数据。 loc只能使用字符型标签来索引数据，不能使用数字来索引数据。特殊情况：当dataframe的行标签或列标签为数字时，loc就可以来索引 行标签和列标签都是数字的情况1234567891011a = np.arange(12).reshape(3, 4)print(\"a: \\n\", a)df = pd.DataFrame(a)print(\"df: \\n\", df)print(\"df.loc[0]: \\n\", df.loc[0])print(\"df.iloc[0]: \\n\", df.iloc[0])print(\"df.loc[:,[0,3]]: \\n\", df.loc[:, [0, 3]])print(\"df.iloc[:, [0,3]]: \\n\", df.iloc[:, [0, 3]]) 将行标签[0, 1, 2]改为[‘a’,’b’,’c’]时的情况 123456789df.index = ['a', 'b', 'c']print(\"df: \\n\", df)# print(df.loc[0]) 报错！TypeError: cannot do label indexing on &lt;class 'pandas.core.indexes.base.Index'&gt; with these indexers [0] of &lt;class 'int'&gt;print(\"df.iloc[0]: \\n\", df.iloc[0])print(\"df.loc['a']: \\n\", df.loc['a'])# print(\"df.iloc['a']: \\n\", df.iloc['a']) 报错！ 将列标签[0, 1, 2]改为[‘A’, ‘B’, ‘C’]时的情况 12345df.columns = ['A', 'B', 'C']print(\"df: \\n\", df)print(\"df.loc[:, 'A']: \\n\", df.loc[:, 'A'])# print(\"df.iloc[:, 'A']: \\n\", df.iloc[:, 'A']) 报错！ ix是一种混合索引，字符型标签和整型索引都可以使用1234print(\"df.ix[0]: \\n\", df.ix[0])print(\"df.ix['a']: \\n\", df.ix['a'])print(\"df.ix[:, 0]: \\n\", df.ix[:, 0])print(\"df.ix[:, 'A']: \\n\", df.ix[:, 'A']) 三、参考博客CSDN博客链接","categories":[{"name":"数据分析","slug":"数据分析","permalink":"https://cdlwhm1217096231.github.io/categories/数据分析/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://cdlwhm1217096231.github.io/tags/数据分析/"},{"name":"Python3","slug":"Python3","permalink":"https://cdlwhm1217096231.github.io/tags/Python3/"},{"name":"Pandas","slug":"Pandas","permalink":"https://cdlwhm1217096231.github.io/tags/Pandas/"}]},{"title":"计算机网络基础知识点总结","slug":"计算机网络基础知识点总结","date":"2019-07-29T11:37:29.000Z","updated":"2019-07-29T14:15:48.676Z","comments":true,"path":"知识点总结/计算机网络/计算机网络基础知识点总结/","link":"","permalink":"https://cdlwhm1217096231.github.io/知识点总结/计算机网络/计算机网络基础知识点总结/","excerpt":"","text":"0.互联网的组成 边缘部分：所有连接在互联网上的主机（主机指的是所有与网络直接相连的计算机）组成，用户可以直接使用，用来进行主机之间的通信和资源共享。 核心部分：大量的网络与连接这些网络所使用的路由器构成，为边缘部分提供服务。 边缘部分总结 边缘部分主要使用核心部分所提供的服务，使得许多主机之间能够互相通信并进行交换或共享信息。计算机之间的通信指的是主机A上的一个进程与主机B上的另一个进程之间进行通信。通信的方式主要有两种： 客户-服务器方式：即C /S方式。客户端发送服务的请求，服务器是服务的提供方。 P2P：对等连接方式。两台通信的主机之间不区分哪个是客户，哪个是服务端，只要两台主机都运行了对等连接软件就可以进行平等、对等的连接通信。 核心部分总结 核心部分起到特殊作用的是路由器，它是一个专用的计算机，主要作用是分组交换和存储转发的功能。 电路交换：使用在电话机之间的通信，使用电话交换机解决了多个电话机之间通信需要大量的电线的问题。电路交换的过程是：建立连接(开始占用通信资源)—-通话(一直占用通信资源)——释放连接(归还通信资源)。电路交换的特点是：通话期间，通话的两个用户会始终占用通信资源。使用电路交换传输计算机数据时，传输效率往往会很低。因为计算机数据具有突变式的特点，线路上真正用来传输数据的时间往往不到10%,大部分通信线路资源绝大部分时间都被浪费了。整个报文的比特流连续的从源点直达终点 分组交换：采用存储转发的技术，把一个报文（需要发生出去的整块数据）划分成几组分组后再进行传输。将报文划分成更小的等长数据段，然后加上首部(包含一些控制信息)，构成了一个分组，分组的首部称为一个包头。单个分组（只是整个报文的一部分）传送到相邻结点，存储下来后查找转发表，转发到下一个结点。 报文交换：整个报文先传送到相邻结点，全部存储下来后查找转发表，转发到下一个结点。 路由器的工作流程：路由器接收到一个分组后，暂存数据到路由器自己的缓存中即自身的存储器中，然后检查其首部，查找转发表。按照首部中的目的地址，找到合适的接口转发除去，把分组交给下一个路由器。这样一步一步以存储转发的方式，把分组交给最终的目的主机。路由器只是暂存一个分组，不是整个报文。分组在哪段链路上传送时才会占用此段链路上的通信资源，在各分组传输之间的空闲时间，此链路也是可以被其他主机发送的分组使用。计算机网络中的常见硬件设备介绍： 物理层：实现网络互连的主要设备有中继器和HUB(集线器) 数据链路层：实现网络互联的主要设备有二层交换机和网桥 网络层：实现网络互连的主要设备有三层交换机和路由器 传输层（包括传输层）以上：实现网络互连的设备有网关 1.计算机网络体系结构 应用层：应用层是体系结构中的最高层。应用层直接为用户的应用进程程序提供服务。这里的进程就是指正在运行的程序。在因特网中的应用层协议很多，如支持万维网应用的http协议支持电子邮件的SMTP协议，支持文件传送的FTP协议等。 运输层：运输层的任务就是负责向两个主机中进程之间的通信提供服务。由于一个主机可以同时运行多个进程，因此运输层有复用和分用的功能。复用就是多个应用层进程可以同时使用下面运输层的服务，分用就是运输层把收到的信息分别交付给上面应用层中的相应进程。 运输层主要使用下面两个协议： 传输控制协议TCP：面向连接的，数据传输的基本单位是报文段，能够提供可靠的交付 用户数据包协议UDP：无连接的，数据传输的基本单位是用户数据报，不能保证提供可靠的交付，只能提供尽最大努力交付。 网络层： 负责为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组或包进行传送。在TCP/IP体系中，由于网络层使用IP协议，因此分组也叫作IP数据报。 数据链路层：两个主机之间的数据传输，总是在一段一段的链路上传送的。在两个相邻结点之间传送数据时，数据链路层将网络层交下来的IP数据报组装成帧，在两个相邻结点间的链路上透明地传送帧中的数据，每一帧包括数据和必要的控制信息。 物理层：在物理层上传送的数据单位是比特。物理层的任务就是透明地传送比特流。 2.各层的作用和支持的协议 3.物理层 传输数据的基本单位：比特流0和1 数据传输系统：源系统（源点、发送器） —&gt; 传输系统 —&gt; 目的系统（接收器、终点） 通道： 单向通道（单工通道）：只有一个方向通信，没有反方向交互，如广播 双向交替通信（半双工通信）：通信双方都可发消息，但不能同时发送或接收 双向同时通信（全双工通信）：通信双方可以同时发送和接收信息 通道复用技术： 频分复用（FDM，Frequency Division Multiplexing）：不同用户在不同频带，所用用户在同样时间占用不同带宽资源 时分复用（TDM，Time Division Multiplexing）：不同用户在同一时间段的不同时间片，所有用户在不同时间占用同样的频带宽度 波分复用（WDM，Wavelength Division Multiplexing）：光的频分复用 码分复用（CDM，Code Division Multiplexing）：不同用户使用不同的码，可以在同样时间使用同样频带通信 4.数据链路层 主要信道： 点对点信道 广播信道 点对点信道： 数据单元：帧 三个基本问题： 封装成帧：把网络层的 IP 数据报封装成帧，SOH - 数据部分 - EOT 透明传输：不管数据部分什么字符，都能传输出去；可以通过字节填充方法解决（冲突字符前加转义字符） 差错检测：降低误码率（BER，Bit Error Rate），广泛使用循环冗余检测（CRC，Cyclic Redundancy Check） 点对点协议（Point-to-Point Protocol）：用户计算机和 ISP 通信时所使用的协议 广播信道： 硬件地址（物理地址、MAC 地址） 单播（unicast）帧（一对一）：收到的帧的 MAC 地址与本站的硬件地址相同 广播（broadcast）帧（一对全体）：发送给本局域网上所有站点的帧 多播（multicast）帧（一对多）：发送给本局域网上一部分站点的帧 5.网络层 IP（Internet Protocol，网际协议）是为计算机网络相互连接进行通信而设计的协议。 ARP（Address Resolution Protocol，地址解析协议） ICMP（Internet Control Message Protocol，网际控制报文协议） IGMP（Internet Group Management Protocol，网际组管理协议） 5.1 IP网际协议 IP地址({&lt;网络号&gt;,&lt;主机号&gt;})分类： IP数据报格式： 5.2 ICMP网际控制报文协议 ICMP报文格式： 应用： PING（Packet InterNet Groper，分组网间探测）测试两个主机之间的连通性 TTL（Time To Live，生存时间）该字段指定 IP 包被路由器丢弃之前允许通过的最大网段数量 5.3 内部网关协议 RIP（Routing Information Protocol，路由信息协议） OSPF（Open Sortest Path First，开放最短路径优先） 5.4 外部网关协议 BGP（Border Gateway Protocol，边界网关协议） 5.5 IP多播 IGMP（Internet Group Management Protocol，网际组管理协议） 多播路由选择协议 5.6 VPN和NAT VPN（Virtual Private Network，虚拟专用网） NAT（Network Address Translation，网络地址转换） 5.7 路由表包含什么？ 网络 ID（Network ID, Network number）：就是目标地址的网络 ID。 子网掩码（subnet mask）：用来判断 IP 所属哪个子网络 下一跳地址/接口（Next hop / interface）：就是数据在发送到目标地址的旅途中下一站的地址。其中 interface 指向 next hop（即为下一个 route）。一个自治系统（AS, Autonomous system）中的 route 应该包含区域内所有的子网络，而默认网关（Network id: 0.0.0.0, Netmask: 0.0.0.0）指向自治系统的出口。 根据应用和执行的不同，路由表可能含有如下附加信息： 花费（Cost）：就是数据发送过程中通过路径所需要的花费 路由的服务质量 路由中需要过滤的出/入连接列表 6.传输层 支持的协议： TCP（Transmission Control Protocol，传输控制协议） UDP（User Datagram Protocol，用户数据报协议） 端口号： 6.1 TCP（Transmission Control Protocol，传输控制协议） TCP是一种面向连接的、可靠的、基于字节流的传输层通信协议，其传输的单位是报文段。 特征： 面向连接 只能点对点（一对一）通信 可靠交互 全双工通信 面向字节流 TCP如何保证可靠传输？ 确认和超时重传 数据合理分片和排序 流量控制 拥塞控制 数据校验 TCP报文结构 TCP首部 TCP：状态控制码（Code，Control Flag），占 6 比特，含义如下： URG：紧急比特（urgent），当 URG＝1 时，表明紧急指针字段有效，代表该封包为紧急封包。它告诉系统此报文段中有紧急数据，应尽快传送(相当于高优先级的数据)， 且上图中的 Urgent Pointer 字段也会被启用。 ACK：确认比特（Acknowledge）。只有当 ACK＝1 时确认号字段才有效，代表这个封包为确认封包。当 ACK＝0 时，确认号无效。 PSH：（Push function）若为 1 时，代表要求对方立即传送缓冲区内的其他对应封包，而无需等缓冲满了才送。 RST：复位比特(Reset)，当 RST＝1 时，表明 TCP 连接中出现严重差错（如由于主机崩溃或其他原因），必须释放连接，然后再重新建立运输连接。 SYN：同步比特(Synchronous)，SYN 置为 1，就表示这是一个连接请求或连接接受报文，通常带有 SYN 标志的封包表示『主动』要连接到对方的意思。 FIN：终止比特(Final)，用来释放一个连接。当 FIN＝1 时，表明此报文段的发送端的数据已发送完毕，并要求释放传输连接。 6. 2 UDP（User Datagram Protocol，用户数据报协议） UDP是 OSI（Open System Interconnection 开放式系统互联） 参考模型中一种无连接的传输层协议，提供面向事务的简单不可靠信息传送服务，其传输的单位是用户数据报。 特征： 无连接 尽最大努力交付 面向报文 没有拥塞控制 支持一对一、一对多、多对多的交互通信 首部开销小 UDP报文结构 UDP首部 6.3 TCP与UDP的区别 TCP面向连接、UDP是无连接的； TCP提供可靠的服务、也就是说，通过TCP连接传输的数据是无差错、不丢失、不重复且按序到达；UDP尽最大努力交付，即不保证可靠交付 TCP的逻辑通信信息是全双工的可靠信息；UDP则是不可靠信息 每一条TCP连接只能是点对点的；UDP支持一对多、多对一、多对多的交互通信 TCP面向字节流(可能会出现黏包问题)，实际上是TCP白数据看成一连串无结构的字节流；UDP是面向报文的(不会出现黏包问题) UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低(对实时应用很有用，如IP电话，实时视频会议等) TCP首部开销20字节；UDP的首部开销小，只有8字节 6.4 TCP的黏包问题 出现黏包问题的原因：TCP 是一个基于字节流的传输服务（UDP 基于报文的），“流” 意味着 TCP 所传输的数据是没有边界的，所以可能会出现两个数据包黏在一起的情况。 解决方法： 发送定长包。如果每个消息的大小都是一样的，那么在接收对等方只要累计接收数据，直到数据等于一个定长的数值就将它作为一个消息。 包头加上包体长度。包头是定长的 4 个字节，说明了包体的长度。接收对等方先接收包头长度，依据包头长度来接收包体。 在数据包之间设置边界，如添加特殊符号 \\r\\n 标记。FTP 协议正是这么做的。但问题在于如果数据正文中也含有 \\r\\n，则会误判为消息的边界。 使用更加复杂的应用层协议6.5 TCP流量控制 概念：流量控制（flow control）就是让发送方的发送速率不要太快，要让接收方来得及接收。 方法：利用可变窗口进行流量控制 6.6 TCP拥塞控制 概念：拥塞控制就是防止过多的数据注入到网络中，可以使网络中的路由器或链路不致过载。 解决方法： 慢启动( slow-start ) 拥塞避免( congestion avoidance ) 快重传( fast retransmit ) 快恢复( fast recovery ) 6.7 TCP传输连接管理(重点) 一.TCP建立连接：三次握手 TCP 建立连接全过程解释： 1.客户端发生SYN给服务器，表示客户端向服务器请求建立连接； 2.服务端收到客户端的SYN，并回复SYN+ACK给客户端(同意建立连接)； 3.客户端收到来自服务器的SYN+ACK后，回复ACK给服务端(表示客户端收到了服务端发的同意报文)； 4.服务端收到客户端的ACK，连接已建立，可以进行数据传输。 建立连接的详细过程： a.B的TCP服务器进程首先创建传输控制块TCB,准备接受客户进程的连接请求。然后服务器进程就处于LISTEN(收听)状态，等待客户的连接请求。如有，就做出响应。 b.A的TCP客户进程也是首先创建传输控制模块TCB，然后向B发出连接请求报文段，这时首部中的同步位SYN=1，同时选择一个初始序号seq=x。TCP规定，SYN报文段(即SYN=1的报文段)不能携带数据，但是要消耗一个序号。这时，TCP客户进程进入SYN-SENT(同步已发送)状态。 c.B收到连接请求报文段后，如同意建立连接，则向A发送确认。在确认报文段中应把SYN和ACK位都置1，确认号是ack=x+1,同时也为自己选择一个初始序号seq=y。注意：这个报文段也不能携带数据，但同样要消耗一个序号。这时TCP服务器进程进入SYN-RECV(同步收到)状态。 d.TCP客户进程收到B的确认后，还要向B给出确认。确认报文段的ACK置1，确认号ack=y+1，而自己的序号seq=x+1。TCP标准规定，ACK报文段可以携带数据。但如果不携带数据则不消耗序号。在这种情况下，下一个数据报文段的序号仍然是seq=x+1。这时，TCP的连接已经建立，A进入ESTABLISHED(已建立连接)状态。当B接收到A的确认后，B也进入ESTABLISHED(已建立连接)状态。 Q1：TCP为什么要进行三次握手？ 因为信道不可靠，而 TCP 想在不可靠信道上建立可靠地传输，那么三次通信是理论上的最小值。(而 UDP 则不需建立可靠传输，因此 UDP 不需要三次握手) 因为双方都需要确认对方收到了自己发送的序列号，确认过程最少要进行三次通信 为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误 二.TCP断开连接：四次挥手 TCP 断开连接全过程解释： 1.客户端发送 FIN 给服务器，说明客户端不必发送数据给服务器了（请求释放从客户端到服务器的连接）； 2.服务器接收到客户端发的 FIN，并回复 ACK 给客户端（同意释放从客户端到服务器的连接）； 3.客户端收到服务端回复的 ACK，此时从客户端到服务器的连接已释放（但服务端到客户端的连接还未释放，并且客户端还可以接收数据）； 4.服务端继续发送之前没发完的数据给客户端； 5.服务端发送 FIN+ACK 给客户端，说明服务端发送完了数据（请求释放从服务端到客户端的连接，就算没收到客户端的回复，过段时间也会自动释放）； 6.服务端发送 FIN+ACK 给客户端，说明服务端发送完了数据（请求释放从服务端到客户端的连接，就算没收到客户端的回复，过段时间也会自动释放）； 7.服务端收到客户端的 ACK 后，断开从服务端到客户端的连接 释放连接的详细过程： a.A和B都处于ESTABLISHED状态，A的应用进程首先向其TCP发出连接释放报文段，并停止再发送数据，主动关闭TCP连接。A把连接释放报文段首部的FIN置1，其序列号seq=u，它等于前面已经传送过的数据的最后一个字节的序号加1。这时，A进入FIN-WAIT-1(终止等待1)状态，等待B的确认。注意：TCP规定：FIN报文段即使不携带数据，它也会消耗一个序号。 b.B收到连接释放报文段后即发出确认，确认号是ack=u+1，而这个报文段自己的序号是v，等待B前面已经传送过的数据的最后一个字节的序号加1。然后B就进入CLOSE-WAIT(关闭等待)状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的连接就释放了，这时的TCP连接处于半关闭状态，即A已经没有数据要发送了，但B若发送数据，A仍要接收。也就是说，从B到A这个方向的连接并没有关闭，这个连接可能会持续一段时间。 c.A收到来自B的确认后，就进入FIN-WAIT-2(终止等待2)状态，等待B发出的连接释放报文段。如果B已经没有要向A发送的数据，其应用进程就会通知TCP释放连接。这时B发出的连接释放报文段必须使FIN=1。现假定B的序号为w(在半关闭状态B可能又发送了一些数据)。B还必须重复上次已经发送过的确认号ack=u+1。这时，B就进入LAST-ACK(最后确认状态)，等待A的确认。 d.A在收到B的释放连接报文段后，必须对此发出一个确认。在确认报文段中把ACK置1，确认号ack=w+1，而自己的序号是seq=u+1(根据TCP标准，前面发送过的FIN报文段要消耗一个序号)。然后经过时间等待计时器(TIME-WAIT)设置的时间2MSL后，A才进入到CLOSED状态。时间MSL叫做最长报文段寿命 Q2：TCP 为什么要进行四次挥手？ 因为 TCP 是全双工模式，客户端请求关闭连接后，客户端向服务端的连接关闭（一二次挥手），服务端继续传输之前没传完的数据给客户端（数据传输），服务端向客户端的连接关闭（三四次挥手）。所以 TCP 释放连接时服务器的 ACK 和 FIN 是分开发送的（中间隔着数据传输），而 TCP 建立连接时服务器的 ACK 和 SYN 是一起发送的（第二次握手），所以 TCP 建立连接需要三次，而释放连接则需要四次。 Q3：为什么 TCP 建立连接时可以 ACK 和 SYN 一起发送，而断开连接时则 ACK 和 FIN 分开发送呢？（ACK 和 FIN 分开是指第二次和第三次挥手） 因为客户端请求释放时，服务器可能还有数据需要传输给客户端，因此服务端要先响应客户端 FIN 请求（服务端发送 ACK），然后数据传输，传输完成后，服务端再提出 FIN 请求（服务端发送 FIN）；而连接时则没有中间的数据传输，因此连接时可以 ACK 和 SYN 一起发送。 Q4：为什么客户端断开连接时，最后需要 TIME-WAIT 等待 2MSL 呢？ 1.为了保证客户端发送的最后一个 ACK 报文能够到达服务端。若未成功到达，则服务端超时重传 FIN+ACK 报文段，客户端再重传 ACK，并重新计时。 2.防止已失效的连接请求报文段出现在本连接中。TIME-WAIT 持续 2MSL 可使本连接持续的时间内所产生的所有报文段都从网络中消失，这样可使下次连接中不会出现旧的连接报文段。6.8 TCP有限状态机 7.应用层7.1 DNS(Domain Name System，域名系统) DNS是互联网的一项服务。它作为将域名和 IP 地址相互映射的一个分布式数据库，能够使人更方便地访问互联网。DNS 使用 TCP 和 UDP 端口 53。当前，对于每一级域名长度的限制是 63 个字符，域名总长度则不能超过 253 个字符。 域名 ::= {&lt;三级域名&gt;.&lt;二级域名&gt;.&lt;顶级域名&gt;}，如：blog.huihut.com 7.2 FTP(File Transfer Protocol，文件传输协议) FTP是用于在网络上进行文件传输的一套标准协议，使用客户/服务器模式，使用 TCP 数据报，提供交互式访问，双向传输。 TFTP（Trivial File Transfer Protocol，简单文件传输协议）一个小且易实现的文件传输协议，也使用客户-服务器方式，使用UDP数据报，只支持文件传输而不支持交互，没有列目录，不能对用户进行身份鉴定 7.3 TELNET TELNET 协议是 TCP/IP 协议族中的一员，是 Internet 远程登陆服务的标准协议和主要方式。它为用户提供了在本地计算机上完成远程主机工作的能力。 7.4 HTTP（HyperText Transfer Protocol，超文本传输协议） HTTP是用于从 WWW（World Wide Web，万维网）服务器传输超文本到本地浏览器的传送协议。 7.5 SMTP（Simple Mail Transfer Protocol，简单邮件传输协议） SMTP是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。SMTP 协议属于 TCP/IP 协议簇，它帮助每台计算机在发送或中转信件时找到下一个目的地。它是在 Internet 传输 Email 的标准，是一个相对简单的基于文本的协议。在其之上指定了一条消息的一个或多个接收者（在大多数情况下被确认是存在的），然后消息文本会被传输。可以很简单地通过 Telnet 程序来测试一个 SMTP 服务器，SMTP 使用 TCP 端口 25。 7.6 DHCP（Dynamic Host Configuration Protocol，动态主机设置协议） DHCP是一个局域网的网络协议，使用 UDP 协议工作，主要有两个用途： 用于内部网络或网络服务供应商自动分配 IP 地址给用户 用于内部网络管理员作为对所有电脑作中央管理的手段 7.7 SNMP（Simple Network Management Protocol，简单网络管理协议） SNMP构成了互联网工程工作小组（IETF，Internet Engineering Task Force）定义的 Internet 协议族的一部分。该协议能够支持网络管理系统，用以监测连接到网络上的设备是否有任何引起管理上关注的情况。 8.相关概念8.1 Socket（套接字） Socket 建立网络通信连接至少要一对端口号（Socket）。Socket 本质是编程接口（API），对 TCP/IP 的封装，TCP/IP 也要提供可供程序员做网络开发所用的接口，这就是 Socket 编程接口。 8.2 WWW（World Wide Web，环球信息网，万维网） WWW是一个由许多互相链接的超文本组成的系统，通过互联网访问8.3 URL（Uniform Resource Locator，统一资源定位符） 概念：URL是因特网上标准的资源的地址（Address） 标准格式：协议类型:[//服务器地址[:端口号]][/资源层级UNIX文件路径]文件名[?查询][#片段ID] 完整格式：协议类型:[//[访问资源需要的凭证信息@]服务器地址[:端口号]][/资源层级UNIX文件路径]文件名[?查询][#片段ID] 注意：其中[访问凭证信息@；:端口号；?查询；#片段ID]都属于选填项,可以省略，如：https://github.com/cdlwhm1217096231 9.HTTP详解 概念：HTTP（HyperText Transfer Protocol，超文本传输协议）是一种用于分布式、协作式和超媒体信息系统的应用层协议。HTTP 是万维网的数据通信的基础。 请求方法： 状态码： 1xx：表示通知信息，如请求收到了或正在进行处理 100 Continue：继续，客户端应继续其请求 101 Switching Protocols 切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议，例如，切换到 HTTP 的新版本协议 2xx：表示成功，如接收或知道了 200 OK: 请求成功 3xx：表示重定向，如要完成请求还必须采取进一步的行动 301 Moved Permanently: 永久移动。请求的资源已被永久的移动到新 URL，返回信息会包括新的 URL，浏览器会自动定向到新 URL。今后任何新的请求都应使用新的 URL 代替 4xx：表示客户的差错，如请求中有错误的语法或不能完成 400 Bad Request: 客户端请求的语法错误，服务器无法理解 401 Unauthorized: 请求要求用户的身份认证 403 Forbidden: 服务器理解请求客户端的请求，但是拒绝执行此请求（权限不够） 404 Not Found: 服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置 “您所请求的资源无法找到” 的个性页面 408 Request Timeout: 服务器等待客户端发送的请求时间过长，超时 5xx：表示服务器的差错，如服务器失效无法完成请求 500 Internal Server Error: 服务器内部错误，无法完成请求 503 Service Unavailable: 由于超载或系统维护，服务器暂时的无法处理客户端的请求。延时的长度可包含在服务器的 Retry-After 头信息中 504 Gateway Timeout: 充当网关或代理的服务器，未及时从远端服务器获取请求 10.DNS(域名解析协议) DNS劫持：指用户访问一个被标记的地址时，DNS服务器故意将此地址指向一个错误的IP地址的行为。范例：收到各种推送广告等网站 DNS污染：指的是用户访问一个地址，国内的服务器(非DNS)监控到用户访问的已经被标记地址时，服务器伪装成DNS服务器向用户发回错误的地址的行为。比如不能访问Google、YouTube等。","categories":[{"name":"知识点总结","slug":"知识点总结","permalink":"https://cdlwhm1217096231.github.io/categories/知识点总结/"},{"name":"计算机网络","slug":"知识点总结/计算机网络","permalink":"https://cdlwhm1217096231.github.io/categories/知识点总结/计算机网络/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://cdlwhm1217096231.github.io/tags/计算机网络/"},{"name":"TCP/IP协议栈","slug":"TCP-IP协议栈","permalink":"https://cdlwhm1217096231.github.io/tags/TCP-IP协议栈/"}]},{"title":"Leetcode刷题记录","slug":"Leetcode刷题记录","date":"2019-07-29T11:33:19.000Z","updated":"2019-07-29T11:34:02.147Z","comments":true,"path":"数据结构与算法/Leetcode刷题记录/","link":"","permalink":"https://cdlwhm1217096231.github.io/数据结构与算法/Leetcode刷题记录/","excerpt":"","text":"递归方法和循环方法的对比 递归方法代码实现比较简洁，但是性能不如循环方法，还有可能出现栈溢出的问题。一般情况下优先考虑递归方法来实现！ 搜索路径的题目：一般使用回溯法，回溯法很适合使用递归方法的代码来实现！当要求不能使用递归实现的时候，考虑使用栈模拟递归的过程 求某个问题的最优解时，并且该问题可以拆分为多个子问题时：可以尝试使用动态规划的方法！在使用自上而下的递归思路去分析动态规划问题时，会发现子问题之间存在重叠的更小的子问题。为了避免不必要的重复计算，使用自下而上的循环代码来实现，即把子问题的最优解先计算出来并用数组保存下来，然后基于子问题的解计算大问题的解。 特殊情况：在分解子问题的时候存在某个特殊的选择，采用这个特殊的选择将一定那个得到最优解，则此题目可能适用于贪心算法！ 典型题目的解题思路：在一个已经排好序的数组中查找一个数字或者统计某个数字出现的次数，可以尝试使用二分查找算法！ Q1:给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。注意：答案中不可以包含重复的三元组。 自己写的：暴力解决，时间复杂度太大 12345678910111213141516171819202122class Solution(object): def threeSum(self, nums): nums.sort() result = [] temp = [] for i in range(len(nums)): for j in range(i + 1, len(nums)): for k in range(j + 1, len(nums)): if nums[i] + nums[j] + nums[k] == 0: result.append([nums[i], nums[j], nums[k]]) for i in range(len(result)): if result[i] not in temp: temp.append(result[i]) else: continue return tempif __name__ == \"__main__\": s = Solution() result = s.threeSum([-1, 0, 1, 2, -1, -4, 3, -5, -2, -3]) print(result) 网上大神的解法： 1234567891011121314151617181920212223242526272829303132333435class Solution: def threeSum(self, nums): # 存储结果列表 result = [] # 对nums列表进行排序，无返回值，排序直接改变nums顺序 nums.sort() for i in range(len(nums)): # 因为是升序排列，如果排序后第一个数都大于0，则跳出循环，不可能有为0的三数之和 if nums[i] &gt; 0: break # 排序后相邻两数如果相等，则跳出当前循环继续下一次循环，相同的数只需要计算一次 if i &gt; 0 and nums[i] == nums[i-1]: continue # 记录i的下一个位置 j = i + 1 # 最后一个元素的位置 k = len(nums) - 1 while j &lt; k: # 判断三数之和是否为0 if nums[j] + nums[k] == -nums[i]: # 把结果加入数组中 result.append([nums[i], nums[j], nums[k]]) # 判断j相邻元素是否相等，有的话跳过这个 while j &lt; k and nums[j] == nums[j+1]: j += 1 # 判断后面k的相邻元素是否相等，是的话跳过 while j &lt; k and nums[k] == nums[k-1]: k -= 1 # 没有相等则j+1，k-1，缩小范围 j += 1 k -= 1 # 小于-nums[i]的话还能往后取 elif nums[j] + nums[k] &lt; -nums[i]: j += 1 else: k -= 1 return result Q2:见下图 A2： 123456789101112class Solution:def romanToInt(self, s: str) -&gt; int: d = &#123;'M': 1000,'D': 500 ,'C': 100,'L': 50,'X': 10,'V': 5,'I': 1&#125; result = 0 s_len = len(s) for i in range(s_len-1): if d[s[i]] &lt; d[s[i+1]]: result -= d[s[i]] else: result += d[s[i]] result += d[s[-1]] return result Q3:编写一个函数来查找字符串数组中的最长公共前缀,如果不存在公共前缀，返回空字符串 “”。 A3：仅仅比较最长与最短的字符串，如果存在相同的前缀就返回；不存在就返回一个空字符串。重要的是如何从两个字符串中取相同位置的字符进行比较。 123456789101112131415def longest_str(strs): s1 = min(strs) # 最短字符串 s2 = max(strs) # 最长字符串 for i, v in enumerate(s1): if v != s2[i]: return s2[:i] # 当第一个字符就不相等时,返回s2[:0]=[],执行下面的if语句 if not strs: return \"\"if __name__ == \"__main__\": strs = [\"dog\", \"racecar\", \"car\"] strs1 = [\"flower\", \"flow\", \"flight\"] result = longest_str(strs) result1 = longest_str(strs1) print(result) print(result1) Q4:给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串，判断字符串是否有效。有效字符串需满足：左括号必须用相同类型的右括号闭合;左括号必须以正确的顺序闭合;注意空字符串可被认为是有效字符串 A4:只有完整出现[],{},()的情况才会返回true,同时空字符串也被任何是有效字符串,所以,用空格进行替换[],{},()，然后比较替换后的结果是否是空字符串，不是的话说明不是有效字符串。 123456def is_Valid(s): while(\"&#123;&#125;\" in s or \"()\" in s or \"[]\" in s): s = s.replace(\"&#123;&#125;\", \"\") s = s.replace(\"()\", \"\") s = s.replace(\"[]\", \"\") return s == \"\" Q5:将两个有序链表合并为一个新的有序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 例如，输入：1-&gt;2-&gt;4, 1-&gt;3-&gt;4；输出：1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 A5: 1234567891011121314151617181920struct ListNode&#123; int val; struct ListNode *next;&#125;; // 借助归并排序的思路，递归方法实现struct ListNode* mergeTwoLists(struct ListNode *l1, struct ListNode *l2)&#123; struct ListNode *p; if (!l1) retutn l2; if (!l2) return l1; if(l1-&gt;val &lt; l2-&gt;val)&#123; // 将两个链表中小的元素放在新的链表中，用指针p指向它 p = l1; p-&gt;next = mergeTwoLists(l1-&gt;next, l2); &#125;else&#123; p = l2; p-&gt;next = mergeTwoLists(l2-&gt;next,l1); &#125; return p;&#125; Q6:给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 A6: 12345678910int removeDuplicates(int *nums, int numsSize)&#123; if (numsSize &lt; 2) return numsSize; int i, j=0; for (i=1;i&lt;numsSize;i++)&#123; if(nums[j] != nums[i]) nums[++j]=nums[i]; &#125; return j+1;&#125; Q7:给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val的元素，返回移除后数组的新长度。不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 A7: 12345678int removeElement(int* nums, int numsSize, int val)&#123; int i,j=0; for (i=0;i &lt; numsSize;i++)&#123; if(nums[i] != val) nums[j++] = nums[i]; &#125; return j;&#125; Q8:统计小于非负整数n的质数的个数。例如。n=10,则输出小于10的质数个数是4个，具体是2, 3, 5, 7。 A8:使用厄拉多塞筛法：首先从数字2开始，依次删除2的倍数；接着从3开始，依次删除3的倍数，然后从5开始(因为4是2的倍数，已经被删除了)，依次删除5的倍数。一直循环上面的步骤的n-1即可，然后统计最后剩余的数的个数，即质数的个数。 123456789101112class Solution: def countPrimes(self, n: int) -&gt; int: if n&lt;=2: return 0 isPrime = [1] * n # 生成一个全为1的列表 isPrime[0], isPrime[1] = 0, 0 for i in range(2, int(n**0.5)+1): # 质数：除1和本身外，没有其他的因数。如果有其他因数p,则p*p = n,即p = n**0.5 if isPrime[i] == 1: # 如果i是质数 isPrime[2*i:n:i] = [0] * len(isPrime[i*2:n:i]) # 将i的倍数置为0 # print(i, isPrime) return sum(isPrime) Q9：给定一个由整数组成的非空数组所表示的非负整数，在该数的基础上加一。最高位数字存放在数组的首位， 数组中每个元素只存储一个数字。你可以假设除了整数 0 之外，这个整数不会以零开头。 A9：思路是如果最后一位不是9，而是0到8，就执行普通的最后一位的加1操作；如果最后一位是9，就要考虑向前面一位产生进位标志1，这是此题的关键！ 12345678910111213class Solution: def plusOne(self, digits: List[int]) -&gt; List[int]: flag = False for i in range(len(digits)-1, -1, -1): # 反向遍历list(起点，终点,步长) if digits[i] is 9: flag = True digits[i] = 0 else: digits[i] += 1 return digits if flag: # 防止出现list=[9]的情况 digits.insert(0, 1) return digits Q10:删除链表中等于给定值 val 的所有节点。示例:输入: 1-&gt;2-&gt;6-&gt;3-&gt;4-&gt;5-&gt;6, val = 6 输出: 1-&gt;2-&gt;3-&gt;4-&gt;5 A10: 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* removeElements(ListNode* head, int val) &#123; // 空链表的情况 if(!head)&#123; return nullptr; &#125; // 删除的节点是头节点 while(head-&gt;val == val)&#123; head = head-&gt;next; if(!head)&#123; return nullptr; &#125; &#125; ListNode* pNode = head; ListNode* pCur = head-&gt;next; // 删除的是中间的某个节点 while(pCur)&#123; if(pCur-&gt;val == val)&#123; pNode-&gt;next = pCur-&gt;next; pCur = pCur-&gt;next; &#125;else&#123; pNode = pCur; pCur = pCur-&gt;next; &#125; &#125; return head; &#125;&#125;; Q11：编写一个算法来判断一个数是不是“快乐数”。一个“快乐数”定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是无限循环但始终变不到 1。如果可以变为 1，那么这个数就是快乐数。 A11: 123456789101112131415161718192021222324252627class Solution&#123;public: bool isHappy(int n)&#123; int sum = 0; // 1到9中只有1和7符合快乐数的定义！ if(n == 1 || n==7)&#123; return true; &#125; // 其余不符合的情况，都不是快乐数! if(n&lt;10)&#123; return false; &#125; sum = isHappyCore(n); return isHappy(sum); // 递归判断 &#125;private: int isHappyCore(int n)&#123; // 下面的代码是取一个整数的各个位置上的数，具有一般性，记忆！ int sum = 0 while(n &gt; 0)&#123; int mod = n % 10; sum += mod * nod; n /= 10; &#125; return sum; &#125;&#125; Q12:给定一个排序链表，删除所有重复的元素，使得每个元素只出现一次。 A12： 123456789101112131415161718192021222324252627282930313233/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* deleteDuplicates(ListNode* head) &#123; if(!head || head-&gt;next == nullptr)&#123; return head; &#125; ListNode* pNode = head; // 慢指针 ListNode* pCur = head-&gt;next; // 快指针 while(pNode-&gt;next != nullptr)&#123; if(pNode-&gt;val == pCur-&gt;val)&#123; // 找到重复元素 if(pCur-&gt;next == nullptr)&#123; // 快指针后面若没有元素直接剔除 pNode-&gt;next = nullptr; &#125;else&#123; // 快指针后有元素 pNode-&gt;next = pCur-&gt;next; pCur = pCur-&gt;next; &#125; &#125;else&#123; //元素不相等 pNode = pNode-&gt;next; pCur = pCur-&gt;next; &#125; &#125; return head; &#125;&#125;; Q13：给定两个二叉树，编写一个函数来检验它们是否相同。如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。 A13： 123456789101112131415161718192021222324/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: bool isSameTree(TreeNode* p, TreeNode* q) &#123; if(p==nullptr &amp;&amp; q==nullptr)&#123; return true; &#125; if(p != nullptr &amp;&amp; q != nullptr &amp;&amp; p-&gt;val == q-&gt;val)&#123; return isSameTree(p-&gt;left, q-&gt;left) &amp;&amp; isSameTree(p-&gt;right, q-&gt;right); // 在左右子树上递归实现！ &#125;else&#123; return false; &#125; &#125;&#125;; Q14：给定一个二叉树，检查它是否是镜像对称的。例如，二叉树 [1,2,2,3,4,4,3] 是对称的。 A14： 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */// 如果是对称二叉树，则从左子树开始遍历与从右子树开始遍历时，遍历的结果都相同！class Solution &#123;public: bool isSymmetric(TreeNode* root) &#123; return isMirror(root,root); // 递归实现 &#125; bool isMirror(TreeNode* root1, TreeNode* root2)&#123; if(root1 == nullptr &amp;&amp; root2 == nullptr)&#123; return true; &#125; if(root1 == nullptr || root2 == nullptr)&#123; return false; &#125; return (root1-&gt;val == root2-&gt;val) &amp;&amp; isMirror(root1-&gt;left, root2-&gt;right) &amp;&amp; isMirror(root1-&gt;right, root2-&gt;left); &#125;&#125;; A15：给定一个大小为 n 的数组，找到其中的众数。众数是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。 Q15： 123456789101112131415class Solution: def majorityElement(self, nums: List[int]) -&gt; int: # nums.sort() # nums_len = len(nums) # return nums[nums_len // 2] # 返回中间的数 candidate = None # 摩尔投票法 count = 0 for num in nums: if num == candidate: # 如果数组中的下一个元素num与candidate相同，就不会碰撞，此时count加1 count += 1 elif count &gt; 0: # 如果数组中的下一个元素num与candidate不同，就会发生碰撞，此时count减1，candidate维持上一次的数据 count -= 1 else: candidate, count = num, 1 # 第一次进入循环，candidate是第一个元素，count加1 return candidate A16：实现一个函数，将字符串中的每个空格替换成%20。例如，输入“hello world.”，则输出”hello%20world.” Q16：解题思路：观察出空格替换后原始字符串变长的关系。在原始字符串的基础上进行修改，利用观察出的关系，使用两个指针从后向前移动将字符串从原始字符串复制到新的字符串中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;// 解题思路：在原始字符串的基础上进行修改，注意原始字符串有足够的空间。使用两个指针，发现空格数量与原始字符串增加的长度关系！class Solution&#123;public: void ReplaceSPace(char* str, int len)&#123; if(str == nullptr || len &lt;= 0)&#123; return; &#125; int original_len = 0; int number_blank = 0; int i=0; // 遍历原始字符串，统计空格的数目 while(str[i] != '\\0')&#123; ++original_len; if(str[i] == ' ')&#123; ++number_blank; &#125; ++i; &#125; int new_len = original_len + 2 * number_blank; if(new_len &gt; len)&#123; return; &#125; int original_index = original_len; int new_index = new_len; while(original_index &gt;= 0 &amp;&amp; new_index &gt; original_index)&#123; if(str[original_index] == ' ')&#123; str[new_index--] = '0'; str[new_index--] = '2'; str[new_index--] = '%'; &#125;else&#123; str[new_index--] = str[original_index]; &#125; original_index--; &#125; &#125;&#125;; Q17：单向链表的基础操作：在单向链表的末尾插入一个节点和找到第一个值为value的节点并将其删除 A17：注意不要忘记释放在堆空间上申请的动态内存 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;iostream&gt;using namespace std;struct ListNode&#123; ListNode* m_pNext; int m_pVal;&#125;;// 在链表的末尾插入一个节点void AddNodeToTail(ListNode** pHead, int value)&#123; // 为新插入的节点分配空间 ListNode* pNew = new ListNode(); pNew-&gt;m_pNext = nullptr; pNew-&gt;m_pVal = value; if(pHead == nullptr)&#123; // 空链表 *pHead = pNew; &#125; else&#123; ListNode* pNode = *pHead; while(pNode-&gt;m_pNext != nullptr)&#123; pNode = pNode-&gt;m_pNext; &#125; pNode-&gt;m_pNext = pNew; &#125;&#125;// 找到第一个含某值value的节点并删除此节点void RemoveNode(ListNode** pHead, int value)&#123; if(pHead == nullptr || *pHead == nullptr)&#123; return; &#125; ListNode* pToDeleted = nullptr; if((*pHead)-&gt;m_pVal == value)&#123; // 头节点就是要删除的那个节点 pToDeleted = *pHead; *pHead = (*pHead)-&gt;m_pNext; &#125;else&#123; // 头节点不是要删除的那个节点 ListNode* pNode = *pHead; while(pNode-&gt;m_pNext != nullptr &amp;&amp; pNode-&gt;m_pNext-&gt;m_pVal != value)&#123; // 头节点不是要删除的那个节点，后面的节点也没有出现value，则一直向后查找 pNode = pNode-&gt;m_pNext; &#125; if(pNode-&gt;m_pNext != nullptr &amp;&amp; pNode-&gt;m_pNext-&gt;m_pVal == value)&#123; // 头节点不是要删除的那个节点,后面的节点找到了value，则执行删除操作 pToDeleted = pNode-&gt;m_pNext; pNode-&gt;m_pNext = pNode-&gt;m_pNext-&gt;m_pNext; &#125; &#125; if(pToDeleted != nullptr)&#123; delete pToDeleted; pToDeleted = nullptr; &#125;&#125; Q18：从尾到头反向打印出单向链表 A18：因为单向链表方向不能反过来，如果将指针反过来来实现改变单向链表的方向。但是，这会改变单向链表的数据结构，故在不改变数据结构的基础上，使用栈来实现！1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;stack&gt;using namespace std;struct ListNode&#123; ListNode* m_pNext; int m_pVal;&#125;;// 利用栈这个数据结构，后进先出！因为单向链表方向不能反过来，如果将指针反过来，从而实现改变链表的方向，但是这会改变链表的数据结构，故在不改变数据结构的基础上，使用栈来实现class Solution&#123;public: vector&lt;int&gt; printListFromTailToHead(ListNode* pHead)&#123; stack&lt;int&gt; nodes; vector&lt;int&gt; result; ListNode* pNode = pHead; while(pNode != nullptr)&#123; nodes.push(pNode-&gt;m_pVal); pNode = pNode-&gt;m_pNext; &#125; while(!nodes.empty())&#123; result.push_back(nodes.top()); nodes.pop(); &#125; return result; &#125;&#125;;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://cdlwhm1217096231.github.io/categories/数据结构与算法/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://cdlwhm1217096231.github.io/tags/数据结构/"}]},{"title":"神经网络中的注意力机制总结及PyTorch实战","slug":"神经网络中的注意力机制总结及PyTorch实战","date":"2019-07-29T11:29:05.000Z","updated":"2019-07-29T12:56:37.646Z","comments":true,"path":"深度学习/神经网络中的注意力机制总结及PyTorch实战/","link":"","permalink":"https://cdlwhm1217096231.github.io/深度学习/神经网络中的注意力机制总结及PyTorch实战/","excerpt":"","text":"0.概述 当神经网络来处理大量的输入信息时，也可以借助人脑的注意力机制，只选择一些关键的信息输入进行处理，用来提高神经网络的效率。在目前的神经网络模型中，可以将max pooling和gating机制近似地看作是自下而上的基于显著性的注意力机制。此外，自上而下的聚焦式注意力也是一种有效的信息选择方法。例如：给定一篇很长的文章，然后就此文章的内容进行提问，提出的问题只和文章中某个段落中的一两个句子相关，其余都无关的。为了减小神经网络的计算代价，只需要把相关的片段挑选出来让后续的神经网络来处理，而不需要把所有文章内容都输入到神经网络中。 1.Attention机制基础知识 用$X=\\left[\\mathbf{x}{1}, \\cdots, \\mathbf{x}{N}\\right]$表示N组输入信息，其中每个向量$\\mathbf{x}_{i}, i \\in[1, N]$都表示一组输入信息。为了节省计算资源，不需要将所有的信息都输入到神经网络中，只需要从X中选择一些和任务相关的信息。注意力机制的计算可以分为两步： (1)在所有输入信息上计算注意力分布； (2)根据注意力分布来计算输入信息的加权平均 1.1 注意力分布 为了从N个输入向量$\\left[\\mathbf{x}{1}, \\cdots, \\mathbf{x}{N}\\right]$中选择出与某个特定任务相关的信息，需要引入一个和任务相关的表示，称为查询向量q，并通过一个打分函数来计算每个输入向量和查询向量之间的相关性。 给定一个和任务相关的查询向量q，用注意力变量$z \\in[1, N]$来表示被选择信息的索引位置，即z=i表示选择了第i个输入向量。为了方便计算，下面首先介绍Soft Attention注意力机制。首先计算在给定q和X下，选择第i个输入向量的概率$\\alpha_{i}$ \\begin{aligned} \\alpha_{i} &=p(z=i | X, \\mathbf{q}) \\\\ &=\\operatorname{softmax}\\left(s\\left(\\mathbf{x}_{i}, \\mathbf{q}\\right)\\right) \\\\ &=\\frac{\\exp \\left(s\\left(\\mathbf{x}_{i}, \\mathbf{q}\\right)\\right)}{\\sum_{j=1}^{N} \\exp \\left(s\\left(\\mathbf{x}_{j}, \\mathbf{q}\\right)\\right)} \\end{aligned}其中$\\alpha{i}$称为注意力分布，$S\\left(\\mathbf{x}{i}, \\mathbf{q}\\right)$是注意力打分函数，可以使用下面的几种方法来计算： 加性模型 $s\\left(\\mathbf{x}{i}, \\mathbf{q}\\right)=\\mathbf{v}^{\\mathrm{T}} \\tanh \\left(W \\mathbf{x}{i}+U \\mathbf{q}\\right)$ 点积模型 $s\\left(\\mathbf{x}{i}, \\mathbf{q}\\right)=\\mathbf{x}{i}^{\\mathrm{T}} \\mathbf{q}$ 缩放点积模型 $s\\left(\\mathbf{x}{i}, \\mathbf{q}\\right)=\\frac{\\mathbf{x}{i}^{\\mathrm{T}} \\mathbf{q}}{\\sqrt{d}}$ 双线性模型 $s\\left(\\mathbf{x}{i}, \\mathbf{q}\\right)=\\mathbf{x}{i}^{\\mathrm{T}} W \\mathbf{q}$ 上式中W、U、v是可学习的参数，d是输入向量的维度。理论上，加性模型和点积模型的复杂度差不多，但是点积模型在实现上可以更好地利用矩阵乘积，从而计算效率更高。但当输入向量的维度d比较高，点积模型的值通常有较大的方差，从而导致softmax函数的梯度比较小。因此，缩放点积模型可以很好地解决这个问题。双线性模型可以看做是一种泛化的点积模型。假设 $s\\left(\\mathbf{x}{i}, \\mathbf{q}\\right)=\\mathbf{x}{i}^{\\mathrm{T}} W \\mathbf{q}$中$W=U^{\\mathrm{T}} V$，则双线性模型可以写为$s\\left(\\mathbf{x}{i}, \\mathbf{q}\\right)=\\mathbf{x}{i}^{\\mathrm{T}} U^{\\mathrm{T}} V \\mathbf{q}=(U \\mathbf{x})^{\\mathrm{T}}(V \\mathbf{q})$即分别对x和q进行线性变换后计算点积。相比点积模型，双线性模型在计算相似度时引入了非对称性。 1.2 加权平均 注意力分布$\\alpha_{i}$可以解释为在给定任务相关的查询q时，第i个输入向量受注意的程度。下面采用一种软性的信息选择机制对输入信息进行汇总。 \\begin{aligned} \\operatorname{att}(X, \\mathbf{q}) &=\\sum_{i=1}^{N} \\alpha_{i} \\mathbf{x}_{i} \\\\ &=\\mathbb{E}_{z \\sim p(z | X, \\mathbf{q})}\\left[\\mathbf{x}_{z}\\right] \\end{aligned}上式称为软注意力机制(Soft Attention Mechanism)。下图给出了软注意力机制的示例图： 2.其他类型的注意力机制 2.1 硬注意力机制 上面的公式$\\mathbb{E}{z \\sim p(z | X, \\mathbf{q})}\\left[\\mathbf{x}{z}\\right]$提到的是软注意力机制，其选择的信息是所有输入向量在注意力分布下的期望。此外还有一种注意力是只关注到某一个输入向量，叫做硬注意力机制(Hard Attention Mechanism)。硬注意力机制有两种方法可以实现： (1)选择最高概率的一个输入向量，即 \\operatorname{att}(X, \\mathbf{q})=\\mathbf{x}_{j}其中j为概率最大的输入向量的下标，即\\begin{array}{c}{j=\\arg \\max _{i=1}^{N} a_{i}}\\end{array} (2)通过在注意力分布上随机采样的方式实现 硬注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用反向传播算法进行训练。为了使用反向传播算法进行训练，一般使用软注意力机制。 2.2 键值对注意力 可以使用键值对格式来表示输入信息，其中键用来计算注意力分布$\\alpha{i}$，值用来计算聚合信息。用$(K, V)=\\left[\\left(\\mathbf{k}{1}, \\mathbf{v}{1}\\right), \\cdots,\\left(\\mathbf{k}{N}, \\mathbf{v}_{N}\\right)\\right]$来表示N组输入信息，给定任务相关的查询向量q时，注意力函数为： \\begin{aligned} \\operatorname{att}((K, V), \\mathbf{q}) &=\\sum_{i=1}^{N} \\alpha_{i} \\mathbf{v}_{i} \\\\ &=\\sum_{i=1}^{N} \\frac{\\exp \\left(s\\left(\\mathbf{k}_{i}, \\mathbf{q}\\right)\\right)}{\\sum_{j} \\exp \\left(s\\left(\\mathbf{k}_{j}, \\mathbf{q}\\right)\\right)} \\mathbf{v}_{i} \\end{aligned}其中$s\\left(\\mathbf{k}_{i}, \\mathbf{q}\\right)$是打分函数，1.2节的图中给出了键值对注意力机制的示意图。当K=V时，键值对模式等于普通模式的注意力机制。 2.3 多头注意力 多头注意力(Multi-head Attention)是利用多个查询 $Q=\\left[\\mathbf{q}{1}, \\cdots, \\mathbf{q}{M}\\right]$来平行计算从输入信息中选取多组信息。每个注意 力关注输入信息的不同部分。 \\operatorname{att}((K, V), Q)=\\operatorname{att}\\left((K, V), \\mathbf{q}_{1}\\right) \\oplus \\cdots \\oplus \\operatorname{att}\\left((K, V), \\mathbf{q}_{M}\\right)其中⊕表示向量拼接。 2.4 自注意力模型(Self Attention) 当使用神经网络来处理一个变化长度的向量序列时，通过可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，如下图所示： 基于卷积或循环网络的序列编码都是可以看做是一种局部的编码方式，只建模了输入信息的局部依赖关系。虽然循环网络理论上可以建立长距离依赖关系，但是由于信息传递的容量以及梯度消失问题，实际上也只能建立短距离依赖关系。 如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互；另一种方法是使用全连接网络。全连接网络是一种非常直接的建模远距离依赖的模型，但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。这时，就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型（Self-Attention Model）。 假设输入序列为$X=\\left[\\mathbf{x}{1}, \\cdots, \\mathbf{x}{N}\\right] \\in \\mathbb{R}^{d{1} \\times N}$，输出序列为$H=\\left[\\mathbf{h}{1}, \\cdots, \\mathbf{h}{N}\\right] \\in \\mathbb{R}^{d{2} \\times N}$，则可以通过线性变换得到三组向量序列： \\begin{aligned} Q &=W_{Q} X \\in \\mathbb{R}^{d_{3} \\times N} \\\\ K &=W_{K} X \\in \\mathbb{R}^{d_{3} \\times N} \\\\ V &=W_{V} X \\in \\mathbb{R}^{d_{2} \\times N} \\end{aligned}其中，Q、K、V分别为查询向量序列，键向量序列、值向量序列，$W{Q} \\in \\mathbb{R}^{d{3} \\times d{1}}$、$W{K} \\in \\mathbb{R}^{d{3} \\times d{1}}$、$W{V} \\in \\mathbb{R}^{d{2} \\times d{1}}$分别表示可学习的参数矩阵。根据$\\operatorname{att}((K, V), \\mathbf{q})=\\sum{i=1}^{N} \\alpha{i} \\mathbf{v}{i}$，可以得到输出向量$\\mathbf{h}{i}$：$\\mathbf{h}{i}=\\operatorname{att}\\left((K, V), \\mathbf{q}{i}\\right)=\\sum{j=1}^{N} \\alpha{i j} \\mathbf{v}{j}=\\sum{j=1}^{N} \\operatorname{softmax}\\left(s\\left(\\mathbf{k}{j}, \\mathbf{q}{i}\\right)\\right) \\mathbf{v}{j}$其中，$i, j \\in[1, N]$为输出和输入向量序列的位置，连接权重$\\alpha_{i j}$由注意力机制动态生成。如果使用缩放点积来作为注意力打分函数，输出向量序列可以写为： H=V \\operatorname{softmax}\\left(\\frac{K^{\\mathrm{T}} Q}{\\sqrt{d_{3}}}\\right)其中softmax函数为按列进行归一化的函数。 下图给出了全连接模型和自注意力模型的对比，其中实线表示可学习的权重，虚线表示动态生成的权重。由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列。 自注意力模型可以作为神经网络中的一层来使用，既可以用来替换卷积层和循环层，也可以和它们一起交替使用(例如输入向量X可以是卷积层或循环层的输出)。自注意模型计算的权重$\\alpha{i j}$只依赖于$\\mathbf{q}{i}$和$\\mathbf{k}_{j}$的相关性，从而忽略了输入信息的位置信息。因此，在单独使用时，自注意模型一般需要加入位置编码信息来进行修正。 3.实战———以Seq2Seq网络进行法语到英语的翻译为例进行说明 利用机器翻译中的经典网络结构Seq2Seq(具体结构见参考资料中的文献)，其中包含Encoder编码网络将输入的法语句子进行编码，然后输入到Decoder解码网络进行解码，输出期望得到的英文句子。整个网络的结构如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447from __future__ import unicode_literals, print_function, divisionfrom io import openimport unicodedataimport stringimport reimport randomimport torchimport torch.nn as nnfrom torch import optimimport torch.nn.functional as Fdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# 将法语翻译成英语SOS_token = 0 # 开始的标注EOS_token = 1 # 结束的标注# 辅助类class Lang: def __init__(self, name): self.name = name self.word2index = &#123;&#125; # word----&gt;index self.index2word = &#123;0: \"SOS\", 1: \"EOS\"&#125; # index----&gt;word self.word2count = &#123;&#125; # 稍后用来替换稀有单词，统计每个单词出现的次数 self.n_words = 2 # 统计单词总数 def addSentence(self, sentence): for word in sentence.split(\" \"): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.n_words self.word2count[word] = 1 self.index2word[self.n_words] = word self.n_words += 1 else: self.word2count[word] += 1 # Turn a Unicode string to plain ASCIIdef unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' )# 小写，修剪和删除非字母字符def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r\"([.!?])\", r\" \\1\", s) s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) return s# 加载文件def readLangs(lang1, lang2, reverse=False): print(\"Reading lines.......\") # 读取文件并进行划分成行 lines = open(r\"E://DeepLearning//jupyter_code//dataset//corpus//translation_data//%s-%s.txt\" % (lang1, lang2), encoding='utf-8').\\ read().strip().split(\"\\n\") # 将每行切成一组pairs pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines] # 将其他语言翻译成英语 if reverse: pairs = [list(reversed(p)) for p in pairs] input_lang = Lang(lang2) output_lang = Lang(lang1) else: input_lang = Lang(lang1) output_lang = Lang(lang2) return input_lang, output_lang, pairs# 由于有很多例句，为了能快速训练，我们会将数据集修剪成相对简短的句子。这里最大长度是10个单词（包括结束标点符号）MAX_LENGTH = 10# 英语前缀eng_prefixes = ( \"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s \", \"you are\", \"you re \", \"we are\", \"we re \", \"they are\", \"they re \")def filterPair(p): return len(p[0].split(' ')) &lt; MAX_LENGTH and \\ len(p[1].split(' ')) &lt; MAX_LENGTH and \\ p[1].startswith(eng_prefixes)def filterPairs(pairs): return [pair for pair in pairs if filterPair(pair)]def prepareData(lang1, lang2, reverse=False): input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse) print(\"Read %s sentence pairs\" % len(pairs)) pairs = filterPairs(pairs) print(\"Trimmed to %s sentence pairs\" % len(pairs)) print(\"Counting words...\") for pair in pairs: input_lang.addSentence(pair[0]) output_lang.addSentence(pair[1]) print(\"Counted words:\") print(input_lang.name, input_lang.n_words) print(output_lang.name, output_lang.n_words) return input_lang, output_lang, pairsinput_lang, output_lang, pairs = prepareData('eng', 'fra', True)# print(\"pairs:\\n\", pairs) pairs = [法语,英语]print(random.choice(pairs))# Encoder 部分class EncoderRNN(nn.Module): def __init__(self, input_size, hidden_size): super(EncoderRNN, self).__init__() self.hidden_size = hidden_size # 隐藏状态a的大小 self.embedding = nn.Embedding(input_size, hidden_size) # 词嵌入层 self.gru = nn.GRU(hidden_size, hidden_size) # 多层的GRU def forward(self, input, hidden): embedded = self.embedding(input).view(1, 1, -1) output = embedded output, hidden = self.gru(output, hidden) return output, hidden def initHidden(self): return torch.zeros(1,1, self.hidden_size, device=device)# Decoder部分class DecoderRNN(nn.Module): def __init__(self, hidden_size, output_size): super(DecoderRNN, self).__init__() self.hidden_size = hidden_size self.embedding = nn.Embedding(output_size, hidden_size) self.gru = nn.GRU(hidden_size, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): output = self.embedding(input).view(1, 1, -1) output = F.relu(output) output, hidden = self.gru(output, hidden) output = self.softmax(self.out(output[0])) return output, hidden def initHidden(self): return torch.zeros(1,1,self.hidden_size, device=device)# Attention 部分class AttnDecoderRNN(nn.Module): def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH): super(AttnDecoderRNN, self).__init__() self.hidden_size = hidden_size self.output_size = output_size self.dropout_p = dropout_p self.max_length = max_length self.embedding = nn.Embedding(self.output_size, self.hidden_size) self.attn = nn.Linear(self.hidden_size*2, self.max_length) self.attn_combine = nn.Linear(self.hidden_size*2, self.hidden_size) self.dropout = nn.Dropout(self.dropout_p) self.gru = nn.GRU(self.hidden_size, self.hidden_size) self.out = nn.Linear(self.hidden_size, self.output_size) def forward(self, input, hidden, encoder_outputs): embedded = self.embedding(input).view(1, 1, -1) embedded = self.dropout(embedded) attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) # 注意力权重 attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) # 两个batch之间的矩阵乘法 output = torch.cat((embedded[0], attn_applied[0]), 1) output = self.attn_combine(output).unsqueeze(0) output = F.relu(output) output, hidden = self.gru(output, hidden) output = F.log_softmax(self.out(output[0]), dim=1) return output, hidden, attn_weights # 隐状态初始化 def initHidden(self): return torch.zeros(1, 1, self.hidden_size, device=device)# 训练模型# 准备训练数据def indexesFromSentence(lang, sentence): return [lang.word2index[word] for word in sentence.split(\" \")]def tensorFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) # EOS作为encoder编码器网络的结束标志， SOS作为Decoder解码器网络的开始标志 return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)def tensorsFromPair(pair): input_tensor = tensorFromSentence(input_lang, pair[0]) # pair[0]是法语 targe_tensor = tensorFromSentence(output_lang, pair[1]) # pair[1]是英语 return (input_tensor, targe_tensor)# 开始训练# “tearcher_forcing_ratio将上一时刻的真实目标输出当作下一个时刻的Encoder网络的输入，而不是使用Encoder网络的上一时刻的预测输出作为下一时刻的输入。tearcher_forcing_ratio = 0.5def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH): encoder_hidden = encoder.initHidden() encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() input_length = input_tensor.size(0) target_length = target_tensor.size(0) encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) loss = 0 # encoder部分 for ei in range(input_length): encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden) encoder_outputs[ei] = encoder_output[0, 0] # decoder部分 decoder_input = torch.tensor([[SOS_token]], device=device) decoder_hidden = encoder_hidden use_teacher_foring = True if random.random() &lt; tearcher_forcing_ratio else False # using teacher forcing if use_teacher_foring: for di in range(target_length): decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs) loss += criterion(decoder_output, target_tensor[di]) decoder_input = target_tensor[di] # 不使用teacher forcing,使用上一时刻的输出作为下一时刻的输入 else: for di in range(target_length): decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs) topv, topi = decoder_output.topk(1) decoder_input = topi.squeeze().detach() loss += criterion(decoder_output, target_tensor[di]) if decoder_input.item() == EOS_token: break loss.backward() encoder_optimizer.step() decoder_optimizer.step() return loss.item() / target_length# 辅助函数------记录时间import timeimport mathdef asMinutes(s): m = math.floor(s / 60) s -= m * 60 return \"%dm %ds\" % (m, s)def timeSince(since, percent): now = time.time() s = now - since es = s / (percent) rs = es - s return \"%s (- %s)\" % (asMinutes(s), asMinutes(rs))# 整个训练过程如下： # 开启定时器 # 初始化优化器和loss函数 # 创建training pairs # 开始训练并绘图def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01): start = time.time() # 开启定时器 plot_losses = [] print_loss_total = 0 # Reset every print_every plot_loss_total = 0 # Reset every plot_every encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) # 定义优化算法 decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate) training_pairs = [tensorsFromPair(random.choice(pairs)) # 创建training pairs for i in range(n_iters)] criterion = nn.NLLLoss() # 定义损失函数 for iter in range(1, n_iters + 1): training_pair = training_pairs[iter - 1] input_tensor = training_pair[0] target_tensor = training_pair[1] loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) print_loss_total += loss plot_loss_total += loss if iter % print_every == 0: print_loss_avg = print_loss_total / print_every print_loss_total = 0 print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg)) if iter % plot_every == 0: plot_loss_avg = plot_loss_total / plot_every plot_losses.append(plot_loss_avg) plot_loss_total = 0 showPlot(plot_losses)# 绘制loss曲线import matplotlib.pyplot as pltplt.switch_backend('agg')import matplotlib.ticker as tickerimport numpy as np%matplotlib inlinedef showPlot(points): plt.figure() fig, ax = plt.subplots() # this locator puts ticks at regular intervals loc = ticker.MultipleLocator(base=0.2) ax.yaxis.set_major_locator(loc) plt.plot(points)# 测试阶段--------测试阶段整体与训练阶段类似，但是测试阶段，不用给出target_tensor,只是将decoder网络上一时刻的预测值作为下一时刻的输入值# 当预测值是EOS时，则停止预测def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH): with torch.no_grad(): input_tensor = tensorFromSentence(input_lang, sentence) input_length = input_tensor.size()[0] encoder_hidden = encoder.initHidden() encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) # encoder部分 for ei in range(input_length): encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden) encoder_outputs[ei] += encoder_output[0, 0] decoder_input = torch.tensor([[SOS_token]], device=device) # SOS decoder_hidden = encoder_hidden decoded_words = [] decoder_attentions = torch.zeros(max_length, max_length) # decoder部分 for di in range(max_length): decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs) decoder_attentions[di] = decoder_attention.data topv, topi = decoder_output.data.topk(1) if topi.item() == EOS_token: # 结束时的条件 decoded_words.append('&lt;EOS&gt;') break else: decoded_words.append(output_lang.index2word[topi.item()]) decoder_input = topi.squeeze().detach() return decoded_words, decoder_attentions[:di + 1]# 随机地从训练集中选择pairs,然后在测试集上进行评估def evaluateRandomly(encoder, decoder, n=10): for i in range(n): pair = random.choice(pairs) print('输入:&gt;', pair[0]) print('目标:=', pair[1]) output_words, attentions = evaluate(encoder, decoder, pair[0]) output_sentence = ' '.join(output_words) print('预测:&lt;', output_sentence) print('')# 正式训练开始运行hidden_size = 256encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)trainIters(encoder1, attn_decoder1, 75000, print_every=5000)evaluateRandomly(encoder1, attn_decoder1)# 注意力可视化output_words, attentions = evaluate( encoder1, attn_decoder1, \"je suis trop froid .\")plt.matshow(attentions.numpy());# 增加坐标轴，更加清楚的可视化def showAttention(input_sentence, output_words, attentions): # Set up figure with colorbar fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(attentions.numpy(), cmap='bone') fig.colorbar(cax) # Set up axes ax.set_xticklabels([''] + input_sentence.split(' ') + ['&lt;EOS&gt;'], rotation=90) ax.set_yticklabels([''] + output_words) # Show label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) plt.show()def evaluateAndShowAttention(input_sentence): output_words, attentions = evaluate( encoder1, attn_decoder1, input_sentence) print('input =', input_sentence) print('output =', ' '.join(output_words)) showAttention(input_sentence, output_words, attentions) 4.参考资料 邱锡鹏：《神经网络与深度学习》 Translation with a Sequence to Sequence Network and Attention PyTorch中文文档 Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation Sequence to Sequence Learning with Neural Networks Neural Machine Translation by Jointly Learning to Align and Translate A Neural Conversational Model 本文中的代码及数据下载地址","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://cdlwhm1217096231.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://cdlwhm1217096231.github.io/tags/深度学习/"},{"name":"NLP","slug":"NLP","permalink":"https://cdlwhm1217096231.github.io/tags/NLP/"},{"name":"PyTorch","slug":"PyTorch","permalink":"https://cdlwhm1217096231.github.io/tags/PyTorch/"}]},{"title":"轻松优化Jupyter Notebook:技巧、诀窍、魔法","slug":"轻松优化Jupyter-Notebook-技巧、诀窍、魔法","date":"2019-07-29T11:26:25.000Z","updated":"2019-07-29T12:56:11.661Z","comments":true,"path":"开发工具/轻松优化Jupyter-Notebook-技巧、诀窍、魔法/","link":"","permalink":"https://cdlwhm1217096231.github.io/开发工具/轻松优化Jupyter-Notebook-技巧、诀窍、魔法/","excerpt":"","text":"0.更换主题1234567pip install jupyterthemes# 使用暗黑主题jt -t chesterish# 恢复默认主题jt -r 1.常用技巧1234567ctrl + shift + p # 查看所有的快捷键按钮# 如果在开头加上感叹号，则可以运行bash命令，例如： !pip install numpy# 在某个函数的末尾加上分号来随时限制函数在最后一行代码上的输出ctr + / # 用来注释或者取消代码 2.MarkDown模式在markdown模式下支持latex 例如：$p(A \\mid B) = \\frac{p(B \\mid A)p(A)}{p(B)}$ 3.输出打印1234567# 打印出所有输出from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = \"all\"# 打印最后一行输出from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = \"last_expr\" 4.安装扩展插件 安装Nbextensions pip安装 12pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user Anaconda安装 123conda install -c conda-forge jupyter_contrib_nbextensionsconda install -c conda-forge jupyter_nbextensions_configuratorjupyter contrib nbextension install --user 5.魔法函数 line magic在一条线上使用,以%开头 cell magic # 在整个cell上使用，以%%开头12%lsmagic # 查看所有的魔法函数%env # 查看环境变量 6.文件的导入与导出12345# 在cell中插入外部的py文件%load basic_import.py# 将cell中的代码导出到一个py文件中%%writefile thiscode.py 7.运行与查看导入的文件12345# 运行py文件中的内容%run basic_import.py# 不确定脚本文件中的内容，可以随时显示它%pycat basic_import.py 8.设置自动保存1%autosave 60 # 每60秒自动保存 9.显示图像1%matplotlib inline 10.定时器 %timeit和%%time放在需要指定的语句前，例如：%%time print(“hello python!”)12%%time # 计算给出cell中的代码运行一次所花费的时间%timeit # 多次运行指定的代码计算平均值，使用的是python中的timeit模块 11.运行其他语言的代码 在不同放入kernel中运行代码，在kernel的开头加上下面对应语言的语句才可以使用！12345678910111213%%bash%%HTML%%python%%python2%%python3%%ruby%%perl%%capture%%javascript%%js%%latex%%markdown%%pypy 12.查看变量1234567# 查找全局范围内的所有变量%who%who str # 只查看str类型的变量# 查看执行某个函数花费的实际%prun 语句名# 使用pdb进行调试必须在每个cell的开头，加上%pdb 13.提供高分辨率的图1%config InlineBackend.figure_format = 'retina' 14.选择执行某些cell1%%script false # 在cell的开头加上此句 15.当需要一直运行某段代码时，通过下面的方法提醒我们的代码何时跑完12345678910111213# 预先安装sox：brew install sox (mac上)# Linux/Mac系统上：import osduration = 1 // secondfreq=440os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % (duration,freq))# Windows系统上：import winsoundduration = 1000freq = 440winsound.Beep(freq.duration) 16.参考博客 博客原文","categories":[{"name":"开发工具","slug":"开发工具","permalink":"https://cdlwhm1217096231.github.io/categories/开发工具/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://cdlwhm1217096231.github.io/tags/Python3/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://cdlwhm1217096231.github.io/tags/Ubuntu/"},{"name":"Jupyter Notebook","slug":"Jupyter-Notebook","permalink":"https://cdlwhm1217096231.github.io/tags/Jupyter-Notebook/"}]},{"title":"C++中虚函数可以是内联函数吗？","slug":"C-中虚函数可以是内联函数吗？","date":"2019-07-29T11:24:45.000Z","updated":"2019-07-29T11:25:35.091Z","comments":true,"path":"C/C-中虚函数可以是内联函数吗？/","link":"","permalink":"https://cdlwhm1217096231.github.io/C/C-中虚函数可以是内联函数吗？/","excerpt":"","text":"1.需要注意的几点： 虚函数可以是内联函数，内联是可以修饰虚函数的，但是当虚函数表现多态性的时候不能内联。 内联是在编译期建议编译器内联，而虚函数的多态性在运行期，编译器无法知道运行期调用哪个代码，因此虚函数表现为多态性时（运行期）不可以内联。 inline virtual 唯一可以内联的时候是：编译器知道所调用的对象是哪个类（如 Base::who()），这只有在编译器具有实际对象而不是对象的指针或引用时才会发生。 2.代码实例如下：123456789101112131415161718192021222324252627282930 #include &lt;iostream&gt; using namespace std; // 基类 class Base&#123; public: inline virtual void who()&#123; cout &lt;&lt; \"I am Base\\n\"; &#125; virtual ~Base()&#123;&#125; &#125;; // 派生类 class Derived:public Base&#123; public: inline void who()&#123; // 不写inline时隐式内联 cout &lt;&lt; \"I am Derived\\n\"; &#125; &#125;;int main()&#123; // 此处的虚函数 who()，是通过类（Base）的具体对象（b）来调用的，编译期间就能确定了，所以它可以是内联的，但最终是否内联取决于编译器。 Base b; b.who(); // 此处的虚函数是通过指针调用的，呈现多态性，需要在运行时期间才能确定，所以不能为内联。 Base *bptr = new Derived(); bptr-&gt;who(); // 因为Base有虚析构函数（virtual ~Base() &#123;&#125;），所以 delete 时，会先调用派生类（Derived）析构函数，再调用基类（Base）析构函数，防止内存泄漏。 delete bptr; bptr = nullptr; return 0; &#125;","categories":[{"name":"C++","slug":"C","permalink":"https://cdlwhm1217096231.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://cdlwhm1217096231.github.io/tags/C/"},{"name":"编程语言","slug":"编程语言","permalink":"https://cdlwhm1217096231.github.io/tags/编程语言/"}]},{"title":"C++中的volatile关键字","slug":"C-中的volatile关键字","date":"2019-07-29T11:22:42.000Z","updated":"2019-07-29T11:24:11.136Z","comments":true,"path":"C/C-中的volatile关键字/","link":"","permalink":"https://cdlwhm1217096231.github.io/C/C-中的volatile关键字/","excerpt":"","text":"1.使用1volatile int i = 10; 2.使用volatile时要几个注意的点： volatile 关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素（操作系统、硬件、其它线程等）更改。所以使用 volatile 告诉编译器不应对这样的对象进行优化。 volatile 关键字声明的变量，每次访问时都必须从内存中取出值（没有被 volatile 修饰的变量，可能由于编译器的优化，从 CPU 寄存器中取值） const 可以是 volatile （如只读的状态寄存器） 指针可以是 volatile","categories":[{"name":"C++","slug":"C","permalink":"https://cdlwhm1217096231.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://cdlwhm1217096231.github.io/tags/C/"},{"name":"编程语言","slug":"编程语言","permalink":"https://cdlwhm1217096231.github.io/tags/编程语言/"}]},{"title":"C++面试知识点总结","slug":"C-面试知识点总结","date":"2019-07-29T11:11:02.000Z","updated":"2019-07-29T14:15:32.197Z","comments":true,"path":"知识点总结/C-面试知识点总结/","link":"","permalink":"https://cdlwhm1217096231.github.io/知识点总结/C-面试知识点总结/","excerpt":"","text":"一.static关键字的作用 1.修饰普通变量，修改变量的存储区域和生命周期，使变量存储在静态区，在 main 函数运行前就分配了空间，如果有初始值就用初始值初始化它，如果没有初始值系统用默认值初始化它。 2.修饰普通函数，表明函数的作用范围，仅在定义该函数的文件内才能使用。在多人开发项目时，为了防止与他人命名空间里的函数重名，可以将函数定位为 static。 3.修饰成员变量，修饰成员变量使所有的对象只保存一个该变量，而且不需要生成对象就可以访问该成员。 4.修饰成员函数，修饰成员函数使得不需要生成对象就可以访问该函数，但是在 static 函数内不能访问非静态成员。 二.C++和C的区别 设计思想上： C++是面向对象的语言，而C是面向过程的结构化编程语言 语法上： C++具有重载、继承和多态三种特性 C++相比C，增加多许多类型安全的功能，比如强制类型转换 C++支持范式编程，比如模板类、函数模板等 三.c++中四种cast转换 C++中四种类型转换是：static_cast, dynamic_cast, const_cast, reinterpret_cast 1.const_cast:对于未定义const版本的成员函数，我们通常需要使用const_cast来去除const引用对象的const，完成函数调用。另外一种使用方式，结合static_cast，可以在非const版本的成员函数内添加const，调用完const版本的成员函数后，再使用const_cast去除const限定。 2.static_cast:完成基础数据类型；同一个继承体系中类型的转换；任意类型与空指针类型void* 之间的转换。 3.dynamic_cast:用于动态类型转换。只能用于含有虚函数的类，用于类层次间的向上(指的是子类向基类的转换)和向下转化(指的是基类向子类的转换)。只能转指针或引用。向下转化时，如果是非法的对于指针返回NULL，对于引用抛异常。它通过判断在执行到该语句的时候变量的运行时类型和要转换的类型是否相同来判断是否能够进行向下转换。 4.reinterpret_cast:几乎什么都可以转，比如将int转指针，可能会出问题，尽量少用； 四.C/C++ 中指针和引用的区别？ 1.指针有自己的一块空间，而引用只是一个别名； 2.使用sizeof看一个指针的大小是4，而引用则是被引用对象的大小； 3.指针可以被初始化为NULL，而引用必须被初始化且必须是一个已有对象 的引用； 4.作为参数传递时，指针需要被解引用才可以对对象进行操作，而直接对引 用的修改都会改变引用所指向的对象； 5.可以有const指针，但是没有const引用； 6.指针在使用中可以指向其它对象，但是引用只能是一个对象的引用，不能 被改变； 7.指针可以有多级指针（**p），而引用只有一级 8.指针和引用使用++运算符的意义不一样； 9.如果返回动态内存分配的对象或者内存，必须使用指针，引用可能引起内存泄露 五.c++中的四个智能指针： shared_ptr,unique_ptr,weak_ptr,auto_ptr 智能指针出现的原因：智能指针的作用是管理一个指针，因为存在以下这种情况：申请的空间在函数结束时忘记释放，造成内存泄漏。使用智能指针可以很大程度上的避免这个问题，因为智能指针就是一个类，当超出了类的作用域是，类会自动调用析构函数，析构函数会自动释放资源。所以智能指针的作用原理就是在函数结束时自动释放内存空间，不需要手动释放内存空间。 1.auto_ptr（c++98的方案，c++11已经抛弃）原因是缺乏语言特性如 “针对构造和赋值” 的 std::move 语义，以及其他瑕疵。 2.unique_ptr（替换auto_ptr）：是 C++11 才开始提供的类型，是一种在异常时可以帮助避免资源泄漏的智能指针。采用独占式拥有，意味着可以确保一个对象和其相应的资源同一时间只被一个 pointer 拥有。一旦拥有着被销毁或编程 empty，或开始拥有另一个对象，先前拥有的那个对象就会被销毁，其任何相应资源亦会被释放。实现独占式拥有（exclusive ownership）或严格拥有（strict ownership）概念，保证同一时间内只有一个智能指针可以指向该对象。你可以移交拥有权。它对于避免内存泄漏（resource leak）——如 new 后忘记 delete ——特别有用。unique_ptr 用于取代 auto_ptr 3.shared_ptr：shared_ptr实现共享式拥有概念。多个智能指针指向相同对象，该对象和其相关资源会在 “最后一个 reference 被销毁” 时被释放。为了在结构较复杂的情景中执行上述工作，标准库提供 weak_ptr、bad_weak_ptr 和 enable_shared_from_this 等辅助类。多个智能指针可以共享同一个对象，对象的最末一个拥有着有责任销毁对象，并清理与该对象相关的所有资源。 4.weak_ptr：weak_ptr 允许你共享但不拥有某对象，一旦最末一个拥有该对象的智能指针失去了所有权，任何 weak_ptr 都会自动成空（empty）。因此，在 default 和 copy 构造函数之外，weak_ptr 只提供 “接受一个 shared_ptr” 的构造函数。可打破环状引用（cycles of references，两个其实已经没有被使用的对象彼此互指，使之看似还在 “被使用” 的状态）的问题。 六.野指针 野指针就是指向一个已删除的对象或者未申请访问受限内存区域的指针 七.为什么析构函数必须是虚函数？为什么C++默认的析构函数不是虚函数 将可能会被继承的父类的析构函数设置为虚函数，可以保证当我们new一个子类，然后使用基类指针指向该子类对象，释放基类指针时可以释放掉子类的空间，防止内存泄漏。 C++默认的析构函数不是虚函数是因为虚函数需要额外的虚函数表和虚表指针，占用额外的内存。而对于不会被继承的类来说，其析构函数如果是虚函数，就会浪费内存。因此C++默认的析构函数不是虚函数，而是只有当需要当作父类时，设置为虚函数。 八.函数指针 1.定义：函数指针是指向函数的指针变量。函数指针本身首先是一个指针变量，该指针变量指向一个具体的函数。这正如用指针变量可指向整型变量、字符型、数组一样，这里是指向函数。C在编译时，每一个函数都有一个入口地址，该入口地址就是函数指针所指向的地址。有了指向函数的指针变量后，可用该指针变量调用函数，就如同用指针变量可引用其他类型变量一样，在这些概念上是大体一致的。 2.用途：调用函数和做函数的参数，比如回调函数。 3.示例： 1234char * fun(char * p) &#123;…&#125; // 指针函数funchar * (*pf)(char * p); // 函数指针pfpf = fun; // 函数指针pf指向函数funpf(p); // 通过函数指针pf调用函数fun 九.fork函数的作用 Fork：创建一个和当前进程映像一样的进程可以通过fork( )系统调用，如下所示 123#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;pid_t fork(void); 成功调用fork( )会创建一个新的进程，它几乎与调用fork( )的进程一模一样，这两个进程都会继续运行。在子进程中，成功的fork( )调用会返回0。在父进程中fork( )返回子进程的pid。如果出现错误，fork( )返回一个负值。 最常见的fork( )用法是创建一个新的进程，然后使用exec( )载入二进制映像，替换当前进程的映像。这种情况下，派生（fork）了新的进程，而这个子进程会执行一个新的二进制可执行文件的映像。这种“派生加执行”的方式是很常见的。 十.C++中析构函数的作用 析构函数与构造函数对应，当对象结束其生命周期，如对象所在的函数已调用完毕时，系统会自动执行析构函数。 析构函数名也应与类名相同，只是在函数名前面加一个位取反符~，例如~stud( )，以区别于构造函数。它不能带任何参数，也没有返回值（包括void类型）。只能有一个析构函数，不能重载。 如果用户没有编写析构函数，编译系统会自动生成一个缺省的析构函数（即使自定义了析构函数，编译器也总是会为我们合成一个析构函数，并且如果自定义了析构函数，编译器在执行时会先调用自定义的析构函数再调用合成的析构函数），它也不进行任何操作，所以许多简单的类中没有用显式的析构函数。 如果一个类中有指针，且在使用的过程中动态的申请了内存，那么最好显式构造析构函数在销毁类之前，释放掉申请的内存空间，避免内存泄漏。 类析构顺序：1）派生类本身的析构函数；2）对象成员析构函数；3）基类析构函数。 十一.静态函数和虚函数的区别 静态函数在编译的时候就已经确定运行时机，虚函数在运行的时候动态绑定。虚函数因为用了虚函数表机制，调用的时候会增加一次内存开销。 十二.重载和重写 重载：两个函数名相同，但是参数列表不同（个数，类型），返回值类型没有要求，在同一作用域中 重写：子类继承了父类，父类中的函数是虚函数，在子类中重新定义了这个虚函数，这种情况是重写 十三.虚函数和多态 多态的实现主要分为静态多态和动态多态，静态多态主要是重载，在编译的时候就已经确定；动态多态是用虚函数机制实现的，在运行期间动态绑定。举个例子：一个父类类型的指针指向一个子类对象时候，使用父类的指针去调用子类中重写了的父类中的虚函数的时候，会调用子类重写过后的函数，在父类中声明为加了virtual关键字的函数，在子类中重写时候不需要加virtual也是虚函数。 虚函数的实现：在有虚函数的类中，类的最开始部分是一个虚函数表的指针，这个指针指向一个虚函数表，表中放了虚函数的地址，实际的虚函数在代码段(.text)中。当子类继承了父类的时候也会继承其虚函数表，当子类重写父类中虚函数时候，会将其继承到的虚函数表中的地址替换为重新写的函数地址。使用了虚函数，会增加访问内存开销，降低效率。 十四.下面四个代码的区别const char * arr = “123”; char * brr = “123”; const char crr[] = “123”; char drr[] = “123”; const char * arr = “123”; // 字符串123保存在常量区，const本来是修饰arr指向的值不能通过arr去修改，但是字符串“123”在常量区，本来就不能改变，所以加不加const效果都一样 char * brr = “123”; // 字符串123保存在常量区，这个brr指针指向的是同一个位置，同样不能通过brr去修改”123”的值 const char crr[] = “123”; // 这里123本来是在栈上的，但是编译器可能会做某些优化，将其放到常量区 char drr[] = “123”; // 字符串123保存在栈区，可以通过drr去修改 十五.const修饰成员函数的目的是什么？ const修饰的成员函数表明函数调用不会对对象做出任何更改，事实上，如果确认不会对对象做更改，就应该为函数加上const限定，这样无论const对象还是普通对象都可以调用该函数。 十六.C++里是怎么定义常量的？常量存放在内存的哪个位置？ 对于局部常量，存放在栈区；对于全局常量，编译期一般不分配内存，放在符号表中以提高访问效率；字面值常量，比如字符串，放在常量区。 十七.new/delete与malloc/free的区别是什么 首先，new/delete是C++的关键字，而malloc/free是C语言的库函数；后者使用必须指明申请内存空间的大小，对于类类型的对象，后者不会调用构造函数和析构函数。 十八.虚函数表具体是怎样实现运行时多态的? 子类若重写父类虚函数，虚函数表中，该函数的地址会被替换，对于存在虚函数的类的对象，在VS中，对象模型的头部存放指向虚函数表的指针，通过该机制实现多态。 十九.C语言是怎么进行函数调用的？ 每一个函数调用都会分配函数栈，在栈内进行函数执行过程。调用前，先把返回地址压栈，然后把当前函数的esp指针压栈。 二十.C++如何处理返回值？ 生成一个临时变量，把它的引用作为函数参数传入函数内。 二十一.C++中拷贝赋值函数的形参能否进行值传递？ 不能。如果是这种情况下，调用拷贝构造函数的时候，首先要将实参传递给形参，这个传递的时候又要调用拷贝构造函数。。如此循环，无法完成拷贝，栈也会满。 二十二.malloc与new区别 malloc需要给定申请内存的大小，返回的指针需要强转；new会调用构造函数，不用指定内存大小，返回的指针不用强转。 二十三.fork,wait,exec函数的作用 父进程产生子进程使用fork拷贝出来一个父进程的副本，此时只拷贝了父进程的页表，两个进程都读同一块内存，当有进程写的时候使用写实拷贝机制分配内存；exec函数可以加载一个elf文件去替换父进程，从此父进程和子进程就可以运行不同的程序了。fork从父进程返回子进程的pid，从子进程返回0；调用了wait的父进程将会发生阻塞，直到有子进程状态改变,执行成功返回0，错误返回-1。exec执行成功则子进程从新的程序开始运行，无返回值，执行失败返回-1。 二十四.C++中类成员的访问权限 C++通过 public、protected、private 三个关键字来控制成员变量和成员函数的访问权限，它们分别表示公有的、受保护的、私有的，被称为成员访问限定符。在类的内部（定义类的代码内部），无论成员被声明为 public、protected 还是 private，都是可以互相访问的，没有访问权限的限制。在类的外部（定义类的代码之外），只能通过对象访问成员，并且通过对象只能访问 public 属性的成员，不能访问 private、protected 属性的成员。 二十五. C++中struct和class的区别 总的来说，struct 更适合看成是一个数据结构的实现体，class 更适合看成是一个对象的实现体。 区别：最本质的一个区别就是默认的访问控制 默认的继承访问权限。struct 是 public 的，class 是 private 的 struct 作为数据结构的实现体，它默认的数据访问控制是 public 的，而 class 作为对象的实现体，它默认的成员变量访问控制是 private 的。 二十六.C++类的内部可以定义引用数据成员吗？ 可以，必须通过成员函数初始化列表初始化 12345678910class MyClass&#123;public: MyClass(int &amp;i): a(1), b(i)&#123; // 构造函数初始化列表中是初始化工作 // 在这里做的是赋值而非初始化工作 &#125;private: const int a; int &amp;b; // 引用数据成员b,必须通过列表初始化！&#125;; 二十七.什么是右值引用，跟左值又有什么区别？ 左值：能对表达式取地址、或具名对象/变量。一般指表达式结束后依然存在的持久对象。 右值：不能对表达式取地址，或匿名对象。一般指表达式结束就不再存在的临时对象。 右值引用和左值引用的区别： 1.左值可以寻址，而右值不可以； 2.左值可以被赋值，右值不可以被赋值，可以用来给左值赋值； 3.左值可变,右值不可变（仅对基础类型适用，用户自定义类型右值引用可以通过成员函数改变）。 二十八.C++源文件从文本到可执行文件经历的过程？ 对于C++源文件，从文本到可执行文件一般需要四个过程： 预处理阶段：对源代码文件中文件包含关系（头文件）、预编译语句（宏定义）进行分析和替换，生成预编译文件。 编译阶段：将经过预处理后的预编译文件转换成特定汇编代码，生成汇编文件 汇编阶段：将编译阶段生成的汇编文件转化成机器码，生成可重定位目标文件 链接阶段：将多个目标文件及所需要的库连接成最终的可执行目标文件 二十九.include头文件的顺序以及双引号””和尖括号&lt;&gt;的区别？ include头文件的顺序：对于include的头文件来说，如果在文件a.h中声明一个在文件b.h中定义的变量，而不引用b.h。那么要在a.c文件中引用b.h文件，并且要先引用b.h，后引用a.h,否则汇报变量类型未声明错误。 双引号和尖括号的区别：编译器预处理阶段查找头文件的路径不一样。对于使用双引号包含的头文件，查找头文件路径的顺序为：当前头文件目录、编译器设置的头文件路径（编译器可使用-I显式指定搜索路径）、系统变量CPLUS_INCLUDE_PATH/C_INCLUDE_PATH指定的头文件路径；对于使用尖括号包含的头文件，查找头文件的路径顺序为：编译器设置的头文件路径（编译器可使用-I显式指定搜索路径）、系统变量CPLUS_INCLUDE_PATH/C_INCLUDE_PATH指定的头文件路径。 三十.什么时候会发生段错误？ 段错误通常发生在访问非法内存地址的时候，具体来说分为以下几种情况： 使用野指针 试图修改字符串常量的内容 三十一.C++11有哪些新特性？ auto关键字：编译器可以根据初始值自动推导出类型，但是不能用于函数传参以及数组类型的推导； nullptr关键字：nullptr是一种特殊类型的字面值，它可以被转换成任意其它的指针类型；而NULL一般被宏定义为0，在遇到重载时可能会出现问题。 智能指针：C++11新增了std::shared_ptr、std::weak_ptr等类型的智能指针，用于解决内存管理的问题。 初始化列表：使用初始化列表来对类进行初始化 右值引用：基于右值引用可以实现移动语义和完美转发，消除两个对象交互时不必要的对象拷贝，节省运算存储资源，提高效率 atomic原子操作用于多线程资源互斥操作 新增STL容器array以及tuple 三十二.const的作用 1.修饰变量，说明该变量不可以被修改 2.修饰指针，分为指向常量的指针(即常量指针)和指针常量 3.常量引用，经常用于形参类型，既避免了拷贝，又避免了函数对值的修改 4.修饰成员函数，说明该成员函数内不能修改成员变量 const用法如下：12345678910111213141516171819202122232425262728293031323334353637383940414243// 类class A&#123;private: const int a; // 常对象成员，只能在初始化列表赋值public: // 构造函数 A() : a(0) &#123; &#125;; A(int x) : a(x) &#123; &#125;; // 初始化列表 // const可用于对重载函数的区分 int getValue(); // 普通成员函数 int getValue() const; // 常成员函数，不得修改类中的任何数据成员的值&#125;;void function()&#123; // 对象 A b; // 普通对象，可以调用全部成员函数、更新常成员变量 const A a; // 常对象，只能调用常成员函数 const A *p = &amp;a; // 常指针 const A &amp;q = a; // 常引用 // 指针 char greeting[] = \"Hello\"; char* p1 = greeting; // 指针变量，指向字符数组变量 const char* p2 = greeting; // 常量指针即常指针，指针的指向可以改变，但是所存的内容不能变 char const* p2 = greeting; // 与const char* p2 等价 char* const p3 = greeting; // 指针常量，指针是一个常量，即指针的指向不能改变，但是指针所存的内容可以改变 const char* const p4 = greeting; // 指向常量的常指针，指针和指针所存的内容都不能改变，本质是一个常量&#125;// 函数void function1(const int Var); // 传递过来的参数在函数内不可变void function2(const char* Var); // 参数为常量指针即指针所指的内容为常量不能变，指针指向可以改变void function3(char* const Var); // 参数为指针常量void function4(const int&amp; Var); // 引用参数在函数内为常量// 函数返回值const int function5(); // 返回一个常数const int* function6(); // 返回一个指向常量的指针变量即常量指针，使用：const int *p = function6();int* const function7(); // 返回一个指向变量的常指针即指针常量，使用：int* const p = function7(); 三十三.this 指针 this 指针是一个隐含于每一个非静态成员函数中的特殊指针。它指向调用该成员函数的那个对象。 当对一个对象调用成员函数时，编译程序先将对象的地址赋给 this 指针，然后调用成员函数，每次成员函数存取数据成员时，都隐式使用 this 指针。 当一个成员函数被调用时，自动向它传递一个隐含的参数，该参数是一个指向这个成员函数所在的对象的指针。 this 指针被隐含地声明为: ClassName *const this，这意味着不能给 this 指针赋值；在 ClassName 类的 const 成员函数中，this 指针的类型为：const ClassName* const，这说明不能对 this 指针所指向的这种对象是不可修改的（即不能对这种对象的数据成员进行赋值操作）； this 并不是一个常规变量，而是个右值，所以不能取得 this 的地址（不能 &amp;this）。在以下场景中，经常需要显式引用 this 指针： 为实现对象的链式引用； 为避免对同一对象进行赋值操作； 在实现一些数据结构时，如 list。 三十四.inline内联函数 内联函数的特点： 相当于把内联函数里面的内容写在调用内联函数处； 相当于不用执行进入函数的步骤，直接执行函数体； 相当于宏，却比宏多了类型检查，真正具有函数特性； 编译器一般不内联包含循环、递归、switch 等复杂操作的内联函数； 在类声明中定义的函数，除了虚函数的其他函数都会自动隐式地当成内联函数。 内联函数的使用： 12345678910111213141516171819// 声明1（加 inline，建议使用）inline int functionName(int first, int second,...);// 声明2（不加 inline）int functionName(int first, int second,...);// 定义inline int functionName(int first, int second,...) &#123;/****/&#125;;// 类内定义，隐式内联class A &#123; int doA() &#123; return 0; &#125; // 隐式内联&#125;// 类外定义，需要显式内联class A &#123; int doA();&#125;inline int A::doA() &#123; return 0; &#125; // 需要显式内联 编译器对内联函数的处理步骤: 将 inline 函数体复制到 inline 函数调用点处； 为所用 inline 函数中的局部变量分配内存空间； 将 inline 函数的的输入参数和返回值映射到调用方法的局部变量空间中； 如果 inline 函数有多个返回点，将其转变为 inline 函数代码块末尾的分支（使用 GOTO） 使用内联函数的优缺点: 优点: 内联函数同宏函数一样将在被调用处进行代码展开，省去了参数压栈、栈帧开辟与回收，结果返回等，从而提高程序运行速度。 内联函数相比宏函数来说，在代码展开时，会做安全检查或自动类型转换（同普通函数），而宏定义则不会。 在类中声明同时定义的成员函数，自动转化为内联函数，因此内联函数可以访问类的成员变量，宏定义则不能。 内联函数在运行时可调试，而宏定义不可以。 缺点: 代码膨胀。内联是以代码膨胀（复制）为代价，消除函数调用带来的开销。如果执行函数体内代码的时间，相比于函数调用的开销较大，那么效率的收获会很少。另一方面，每一处内联函数的调用都要复制代码，将使程序的总代码量增大，消耗更多的内存空间。 inline 函数无法随着函数库升级而升级。inline函数的改变需要重新编译，不像 non-inline 可以直接链接。 是否内联，程序员不可控。内联函数只是对编译器的建议，是否对函数内联，决定权在于编译器。 虚函数可以是内联函数吗？ 虚函数可以是内联函数，内联是可以修饰虚函数的，但是当虚函数表现多态性的时候不能内联。 内联是在编译器建议编译器内联，而虚函数的多态性在运行期，编译器无法知道运行期调用哪个代码，因此虚函数表现为多态性时（运行期）不可以内联。 inline virtual 唯一可以内联的时候是：编译器知道所调用的对象是哪个类（如 Base::who()），这只有在编译器具有实际对象而不是对象的指针或引用时才会发生。 虚函数内联使用实例如下:12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;using namespace std;class Base&#123;public: inline virtual void who() &#123; cout &lt;&lt; \"I am Base\\n\"; &#125; virtual ~Base() &#123;&#125;&#125;;class Derived : public Base&#123;public: inline void who() // 不写inline时隐式内联 &#123; cout &lt;&lt; \"I am Derived\\n\"; &#125;&#125;;int main()&#123; // 此处的虚函数 who()，是通过类（Base）的具体对象（b）来调用的，编译期间就能确定了，所以它可以是内联的，但最终是否内联取决于编译器。 Base b; b.who(); // 此处的虚函数是通过指针调用的，呈现多态性，需要在运行时期间才能确定，所以不能为内联。 Base *ptr = new Derived(); ptr-&gt;who(); // 因为Base有虚析构函数（virtual ~Base() &#123;&#125;），所以 delete 时，会先调用派生类（Derived）析构函数，再调用基类（Base）析构函数，防止内存泄漏。 delete ptr; ptr = nullptr; system(\"pause\"); return 0;&#125; 三十五.volatile关键字1volatile int i=10; volatile 关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素（操作系统、硬件、其它线程等）更改。所以使用 volatile 告诉编译器不应对这样的对象进行优化。 volatile 关键字声明的变量，每次访问时都必须从内存中取出值（没有被 volatile 修饰的变量，可能由于编译器的优化，从 CPU 寄存器中取值） const 可以是 volatile （如只读的状态寄存器） 指针可以是volatile 三十六.assert() 断言是宏，而非函数。assert 宏的原型定义在 （C）、\\（C++）中，其作用是如果它的条件返回错误，则终止程序执行。可以通过定义 NDEBUG 来关闭 assert，但是需要在源代码的开头，include 之前。 assert()使用1234#define NDEBUG // 加上这行，则 assert 不可用#include &lt;assert.h&gt;assert( p != NULL ); // assert 不可用 三十七.sizeof()运算符 sizeof 对数组，得到整个数组所占空间大小。 sizeof 对指针，得到指针本身所占空间大小。 三十八.#pragma pack(n) 用途：设定结构体、联合以及类成员变量以 n 字节方式对齐 #pragma pack(n)使用实例：1234567891011#pragma pack(push) // 保存对齐状态#pragma pack(4) // 设定为 4 字节对齐struct test&#123; char m1; double m4; int m3;&#125;;#pragma pack(pop) // 恢复对齐状态 三十九. extern “C” 用途：extern “C” 的作用是让 C++ 编译器将 extern “C” 声明的代码当作 C 语言代码处理，可以避免 C++ 因符号修饰导致代码不能和C语言库中的符号进行链接的问题。 被 extern 限定的函数或变量是 extern 类型的；被 extern “C” 修饰的变量和函数是按照 C 语言方式编译和链接的 extern “C”实例如下：123456789#ifdef __cplusplusextern \"C\" &#123;#endifvoid *memset(void *, int, size_t);#ifdef __cplusplus&#125;#endif 四十.struct 和 typedef struct C语言中： 123456789// ctypedef struct Student &#123; int age;&#125; S;// 等价于下面struct Student &#123; int age;&#125; ;typedef struct Student S; C++中： 1.如果在类标识符空间定义了 struct Student {…};，使用 Student me; 时，编译器将搜索全局标识符表，Student 未找到，则在类标识符内搜索。即表现为可以使用 Student 也可以使用 struct Student，如下： 123456// cppstruct Student &#123; int age;&#125;;void f( Student me ); // 正确，\"struct\" 关键字可省略 2.若定义了与 Student 同名函数之后，则 Student 只代表函数，不代表结构体，如下： 12345678910111213typedef struct Student &#123; int age;&#125; S;void Student() &#123;&#125; // 正确，定义后 \"Student\" 只代表此函数//void S() &#123;&#125; // 错误，符号 \"S\" 已经被定义为一个 \"struct Student\" 的别名int main() &#123; Student(); struct Student me; // 或者 \"S me\"; return 0;&#125; 四十一.union联合体 联合（union）是一种节省空间的特殊的类，一个 union 可以有多个数据成员，但是在任意时刻只有一个数据成员可以有值。当某个成员被赋值后其他成员变为未定义状态。联合有如下特点： 默认访问控制符为 public 可以含有构造函数、析构函数 不能含有引用类型的成员 不能继承自其他类，不能作为基类 不能含有虚函数 匿名 union 在定义所在作用域可直接访问 union 成员 匿名 union 不能包含 protected 成员或 private 成员 全局匿名联合必须是静态（static）的 union使用实例如下： 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;union UnionTest &#123; UnionTest() : i(10) &#123;&#125;; int i; double d;&#125;;static union &#123; int i; double d;&#125;;int main() &#123; UnionTest u; union &#123; int i; double d; &#125;; std::cout &lt;&lt; u.i &lt;&lt; std::endl; // 输出 UnionTest 联合的 10 ::i = 20; std::cout &lt;&lt; ::i &lt;&lt; std::endl; // 输出全局静态匿名联合的 20 i = 30; std::cout &lt;&lt; i &lt;&lt; std::endl; // 输出局部匿名联合的 30 return 0;&#125; 四十二.explicit（显式）关键字 explicit 修饰构造函数时，可以防止隐式转换和复制初始化，必须显式初始化 explicit 修饰转换函数时，可以防止隐式转换，但按语境转换 除外 explicit使用实例如下：123456789101112131415161718192021222324252627282930313233343536373839404142struct A&#123; A(int) &#123; &#125; operator bool() const &#123; return true; &#125;&#125;;struct B&#123; explicit B(int) &#123;&#125; explicit operator bool() const &#123; return true; &#125;&#125;;void doA(A a) &#123;&#125;void doB(B b) &#123;&#125;int main()&#123; A a1(1); // OK：直接初始化 A a2 = 1; // OK：复制初始化 A a3&#123; 1 &#125;; // OK：直接列表初始化 A a4 = &#123; 1 &#125;; // OK：复制列表初始化 A a5 = (A)1; // OK：允许 static_cast 的显式转换 doA(1); // OK：允许从 int 到 A 的隐式转换 if (a1); // OK：使用转换函数 A::operator bool() 的从 A 到 bool 的隐式转换 bool a6（a1）; // OK：使用转换函数 A::operator bool() 的从 A 到 bool 的隐式转换 bool a7 = a1; // OK：使用转换函数 A::operator bool() 的从 A 到 bool 的隐式转换 bool a8 = static_cast&lt;bool&gt;(a1); // OK ：static_cast 进行直接初始化 B b1(1); // OK：直接初始化 B b2 = 1; // 错误：被 explicit 修饰构造函数的对象不可以复制初始化 B b3&#123; 1 &#125;; // OK：直接列表初始化 B b4 = &#123; 1 &#125;; // 错误：被 explicit 修饰构造函数的对象不可以复制列表初始化 B b5 = (B)1; // OK：允许 static_cast 的显式转换 doB(1); // 错误：被 explicit 修饰构造函数的对象不可以从 int 到 B 的隐式转换 if (b1); // OK：被 explicit 修饰转换函数 B::operator bool() 的对象可以从 B 到 bool 的按语境转换 bool b6(b1); // OK：被 explicit 修饰转换函数 B::operator bool() 的对象可以从 B 到 bool 的按语境转换 bool b7 = b1; // 错误：被 explicit 修饰转换函数 B::operator bool() 的对象不可以隐式转换 bool b8 = static_cast&lt;bool&gt;(b1); // OK：static_cast 进行直接初始化 return 0;&#125; 四十三.friend友元类和友元函数 能访问私有成员、破坏封装性、友元关系不可传递、友元关系的单向性、友元声明的形式及数量不受限制 四十四.:: 范围解析运算符 种类： 全局作用域符（::name）：用于类型名称（类、类成员、成员函数、变量等）前，表示作用域为全局命名空间 类作用域符（class::name）：用于表示指定类型的作用域范围是具体某个类的 命名空间作用域符（namespace::name）:用于表示指定类型的作用域范围是具体某个命名空间的 使用实例： 1234567891011121314151617int count = 0; // 全局（::）的 countclass A &#123;public: static int count; // 类 A 的 count（A::count）&#125;;int main() &#123; ::count = 1; // 设置全局的 count 的值为 1 A::count = 2; // 设置类 A 的 count 为 2 int count = 0; // 局部的 count count = 3; // 设置局部的 count 的值为 3 return 0;&#125; 四十五.enum枚举类型 限定作用域的枚举类型: 1enum class open_modes &#123; input, output, append &#125;; 不限定作用域的枚举类型: 12enum color &#123; red, yellow, green &#125;;enum &#123; floatPrec = 6, doublePrec = 10 &#125;; 四十六.decltype关键字 作用和用法：用于检查实体的声明类型或表达式的类型及值分类。语法：decltype ( expression ) decltype实例如下： 1234567891011121314// 尾置返回允许我们在参数列表之后声明返回类型template &lt;typename It&gt;auto fcn(It beg, It end) -&gt; decltype(*beg)&#123; // 处理序列 return *beg; // 返回序列中一个元素的引用&#125;// 为了使用模板参数成员，必须用 typenametemplate &lt;typename It&gt;auto fcn2(It beg, It end) -&gt; typename remove_reference&lt;decltype(*beg)&gt;::type&#123; // 处理序列 return *beg; // 返回序列中一个元素的拷贝&#125; 四十七.引用和宏 左值引用：常规引用，一般表示对象的身份 右值引用：右值引用就是必须绑定到右值（一个临时对象、将要销毁的对象）的引用，一般表示对象的值；右值引用可实现转移语义（Move Sementics）和精确传递（Perfect Forwarding），它的主要目的有两个方面： 消除两个对象交互时不必要的对象拷贝，节省运算存储资源，提高效率。 能够更简洁明确地定义泛型函数。 引用折叠： X&amp; &amp;、X&amp; &amp;&amp;、X&amp;&amp; &amp; 可折叠成 X&amp;；X&amp;&amp; &amp;&amp; 可折叠成 X&amp;&amp; 宏：宏定义可以实现类似于函数的功能，但是它终归不是函数，而宏定义中括弧中的“参数”也不是真的参数，在宏展开的时候对 “参数” 进行的是一对一的替换。 四十八.必须使用成员初始化列表的场合 好处：更高效：少了一次调用默认构造函数的过程。 有些场合必须要用初始化列表： 常量成员，因为常量只能初始化不能赋值，所以必须放在初始化列表里面 引用类型，引用必须在定义的时候初始化，并且不能重新赋值，所以也要写在初始化列表里面 没有默认构造函数的类类型，因为使用初始化列表可以不必调用默认构造函数来初始化 四十九.面向对象三大特征 封装：把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。关键字：public, protected, private。不写默认为 private。 public 成员：可以被任意实体访问 protected 成员：只允许被子类及本类的成员函数访问 private 成员：只允许被本类的成员函数、友元类或友元函数访问 继承：基类（父类）——&gt; 派生类（子类） 多态：即多种状态（形态）。简单来说，我们可以将多态定义为消息以多种形式显示的能力。多态是以封装和继承为基础的。 C++ 多态分类及实现： 重载多态（Ad-hoc Polymorphism，编译期）：函数重载、运算符重载 子类型多态（Subtype Polymorphism，运行期）：虚函数 参数多态性（Parametric Polymorphism，编译期）：类模板、函数模板 强制多态（Coercion Polymorphism，编译期/运行期）：基本类型转换、自定义类型转换 静态多态(编译期/早绑定) 函数重载实例: 123456class A&#123;public: void do(int a); void do(int a, int b);&#125;; 动态多态(运行期/晚绑定) 虚函数：用 virtual 修饰成员函数，使其成为虚函数 注意： 普通函数（非类成员函数）不能是虚函数 静态函数（static）不能是虚函数 构造函数不能是虚函数（因为在调用构造函数时，虚表指针并没有在对象的内存空间中，必须要构造函数调用完成后才会形成虚表指针） 内联函数不能是表现多态性时的虚函数 动态多态实例 123456789101112131415161718192021222324252627282930313233class Shape // 形状类&#123;public: virtual double calcArea() &#123; ... &#125; virtual ~Shape();&#125;;class Circle : public Shape // 圆形类&#123;public: virtual double calcArea(); ...&#125;;class Rect : public Shape // 矩形类&#123;public: virtual double calcArea(); ...&#125;;int main()&#123; Shape * shape1 = new Circle(4.0); Shape * shape2 = new Rect(5.0, 6.0); shape1-&gt;calcArea(); // 调用圆形类里面的方法 shape2-&gt;calcArea(); // 调用矩形类里面的方法 delete shape1; shape1 = nullptr; delete shape2; shape2 = nullptr; return 0;&#125; 五十.虚析构函数 虚析构函数是为了解决基类的指针指向派生类对象，并用基类的指针删除派生类对象。 虚析构函数的使用如下: 123456789101112131415161718192021class Shape&#123;public: Shape(); // 构造函数不能是虚函数 virtual double calcArea(); virtual ~Shape(); // 虚析构函数&#125;;class Circle : public Shape // 圆形类&#123;public: virtual double calcArea(); ...&#125;;int main()&#123; Shape * shape1 = new Circle(4.0); shape1-&gt;calcArea(); delete shape1; // 因为Shape有虚析构函数，所以delete释放内存时，先调用子类析构函数，再调用基类析构函数，防止内存泄漏。 shape1 = NULL; return 0；&#125; 五十一.纯虚函数 定义：纯虚函数是一种特殊的虚函数，在基类中不能对虚函数给出有意义的实现，而把它声明为纯虚函数，它的实现留给该基类的派生类去做。 用法： virtual int A() = 0; 五十二.虚函数、纯虚函数 类里如果声明了虚函数，这个函数是实现的，哪怕是空实现，它的作用就是为了能让这个函数在它的子类里面可以被覆盖，这样的话，编译器就可以使用后期绑定来达到多态了。纯虚函数只是一个接口，是个函数的声明而已，它要留到子类里去实现。 虚函数在子类里面也可以不重载的；但纯虚函数必须在子类去实现。 虚函数的类用于 “实作继承”，继承接口的同时也继承了父类的实现。当然大家也可以完成自己的实现。纯虚函数关注的是接口的统一性，实现由子类完成。 带纯虚函数的类叫抽象类，这种类不能直接生成对象，而只有被继承，并重写其虚函数后，才能使用。抽象类被继承后，子类可以继续是抽象类，也可以是普通类。 虚基类是虚继承中的基类。 五十三.虚函数指针、虚函数表 虚函数指针：在含有虚函数类的对象中，指向虚函数表，在运行时确定。 虚函数表：在程序只读数据段，存放虚函数指针，如果派生类实现了基类的某个虚函数，则在虚函数表中覆盖原本基类的那个虚函数指针，在编译时根据类的声明创建。 五十四.虚继承 用途：用于解决多继承条件下的菱形继承问题（浪费存储空间、存在二义性） 底层实现原理与编译器相关，一般通过虚基类指针和虚基类表实现，每个虚继承的子类都有一个虚基类指针（占用一个指针的存储空间，4字节）和虚基类表（不占用类对象的存储空间）（需要强调的是，虚基类依旧会在子类里面存在拷贝，只是仅仅最多存在一份而已，并不是不在子类里面了）；当虚继承的子类被当做父类继承时，虚基类指针也会被继承。实际上，vbptr 指的是虚基类表指针（virtual base table pointer），该指针指向了一个虚基类表（virtual table），虚表中记录了虚基类与本类的偏移地址；通过偏移地址，这样就找到了虚基类成员，而虚继承也不用像普通多继承那样维持着公共基类（虚基类）的两份同样的拷贝，节省了存储空间。 五十五.虚继承、虚函数 相同点：都利用了虚指针（均占用类的存储空间）和虚表（均不占用类的存储空间） 不同点： 虚继承： 虚基类依旧存在继承类中，只占用存储空间 虚基类表存储的是虚基类相对直接继承类的偏移 虚函数： 虚函数不占用存储空间 虚函数表存储的是虚函数地址 五十六.模板类、成员模板、虚函数 模板类中可以使用虚函数 一个类（无论是普通类还是类模板）的成员模板（本身是模板的成员函数）不能是虚函数 五十七.抽象类、接口类、聚合类 抽象类：含有纯虚函数的类 接口类：仅含有纯虚函数的抽象类 聚合类：用户可以直接访问其成员，并且具有特殊的初始化语法形式。满足如下特点： 所有成员都是 public 没有定义任何构造函数 没有类内初始化 没有基类，也没有 virtual 函数 五十八.内存分配和管理 malloc、calloc、realloc、alloca malloc：申请指定字节数的内存。申请到的内存中的初始值不确定。 calloc：为指定长度的对象，分配能容纳其指定个数的内存。申请到的内存的每一位（bit）都初始化为 0。 realloc：更改以前分配的内存长度（增加或减少）。当增加长度时，可能需将以前分配区的内容移到另一个足够大的区域，而新增区域内的初始值则不确定。 alloca：在栈上申请内存。程序在出栈的时候，会自动释放内存。但是需要注意的是，alloca 不具可移植性, 而且在没有传统堆栈的机器上很难实现。alloca 不宜使用在必须广泛移植的程序中。C99 中支持变长数组 (VLA)，可以用来替代 alloca。 malloc和free 用途：用于分配、释放内存 使用： 申请内存，确认是否申请成功 12char *str = (char*) malloc(100);assert(str != nullptr); 释放内存后指针置空 12free(p);p = nullptr; new和delete new / new[]：完成两件事，先底层调用 malloc 分配了内存，然后调用构造函数（创建对象）。 delete/delete[]：也完成两件事，先调用析构函数（清理资源），然后底层调用 free 释放空间。 new 在申请内存时会自动计算所需字节数，而 malloc 则需我们自己输入申请内存空间的字节数。 使用： 123456int main()&#123; T* t = new T(); // 先内存分配 ，再构造函数 delete t; // 先析构函数，再内存释放 return 0;&#125; 五十九.delete this 合法吗？ 合法，但是： 必须保证 this 对象是通过 new（不是 new[]、不是 placement new、不是栈上、不是全局、不是其他对象成员）分配的 必须保证调用 delete this 的成员函数是最后一个调用 this 的成员函数 必须保证成员函数的 delete this 后面没有调用 this 了 必须保证 delete this 后没有人使用了 六十.如何定义一个只能在堆上（栈上）生成对象的类？ 只能在堆上 方法： 将析构函数设置为私有 原因：C++ 是静态绑定语言，编译器管理栈上对象的生命周期，编译器在为类对象分配栈空间时，会先检查类的析构函数的访问性。若析构函数不可访问，则不能在栈上创建对象。 只能在栈上 方法：将 new 和 delete 重载为私有 原因： 在堆上生成对象，使用 new 关键词操作，其过程分为两阶段：第一阶段，使用 new 在堆上寻找可用内存，分配给对象；第二阶段，调用构造函数生成对象。将 new 操作设置为私有，那么第一阶段就无法完成，就不能够在堆上生成对象。 六十一.强制类型转换运算符(4种) static_cast 特点：静态转换，在编译处理期间。 应用场合： 主要用于C++中内置的基本数据类型之间的转换，但是没有运行时类型的检测来保证转换的安全性。 a.用于基类和子类之间的指针或引用之间的转换，这种转换把子类的指针或引用转换为基类表示是安全的；进行下行转换，把积累的指针或引用转换为子类表示时，由于没有进行动态类型检测，所以是不安全的。 b.把void类型的指针转换成目标类型的指针（不安全） c.不能用于两个不相关的类型转换 d.不能把const对象转换成非const对象 const_cast 特点：去常转换，编译时执行。 应用场合： const_cast操作不能在不同的种类间转换。相反，它仅仅把它作用的表达式转换成常量。它可以使一个本来不是const类型的数据转换成const类型的，或者把const属性去掉。 reinterpret_cast: 特点：重解释类型转换 应用场合： 它有着和c风格强制类型转换同样的功能；它可以转化任何的内置数据类型为其他的类型，同时它也可以把任何类型的指针转化为其他的类型；它的机理是对二进制进行重新的解释，不会改变原来的格式。 dynamic_cast &lt; type-id &gt; ( expression ) 特点：该运算符将expression转换成type_id类型的对象。type_id必须是类的指针，类的引用或者空类型的指针。 应用场合： a.如果type_id是一个指针类型，那么expression也必须是一个指针类型，如果type_id是一个引用类型，那么expression也必须是一个引用类型。 b.如果type_id是一个空类型的指针，在运行的时候，就会检测expression的实际类型，结果是一个由expression决定的指针类型。 c.如果type_id不是空类型的指针，在运行的时候指向expression对象的指针能否可以转换成type_id类型的指针 d.在运行的时候决定真正的类型，如果向下转换是安全的，就返回一个转换后的指针，若不安全，则返回一个空指针 e.主要用于上下行之间的转换，也可以用于类之间的交叉转换。上行转换时和static_cast效果一样，下行转换时，具有检测功能，比static_cast更安全。","categories":[{"name":"知识点总结","slug":"知识点总结","permalink":"https://cdlwhm1217096231.github.io/categories/知识点总结/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://cdlwhm1217096231.github.io/tags/C/"},{"name":"编程语言","slug":"编程语言","permalink":"https://cdlwhm1217096231.github.io/tags/编程语言/"}]},{"title":"C++常用参考资料","slug":"C-常用参考资料","date":"2019-07-29T11:09:15.000Z","updated":"2019-07-29T11:10:16.456Z","comments":true,"path":"常用资料/C-常用参考资料/","link":"","permalink":"https://cdlwhm1217096231.github.io/常用资料/C-常用参考资料/","excerpt":"","text":"1.常用参考资料 STL标准模板库的使用 C++参考手册","categories":[{"name":"常用资料","slug":"常用资料","permalink":"https://cdlwhm1217096231.github.io/categories/常用资料/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://cdlwhm1217096231.github.io/tags/C/"},{"name":"编程语言","slug":"编程语言","permalink":"https://cdlwhm1217096231.github.io/tags/编程语言/"}]},{"title":"神经网络中的批量归一化Batch Normalization(BN)原理总结","slug":"神经网络中的批量归一化Batch-Normalization-BN-原理总结","date":"2019-07-29T11:02:46.000Z","updated":"2019-07-29T11:07:14.646Z","comments":true,"path":"深度学习/神经网络中的批量归一化Batch-Normalization-BN-原理总结/","link":"","permalink":"https://cdlwhm1217096231.github.io/深度学习/神经网络中的批量归一化Batch-Normalization-BN-原理总结/","excerpt":"","text":"0.概述 深层神经网络存在的问题(从当前层的输入的分布来分析)：在深层神经网络中，中间层的输入是上一层神经网络的输出。因此，之前的层的神经网络参数的变化会导致当前层输入的分布发生较大的差异。在使用随机梯度下降法来训练神经网络时，每次参数更新都会导致网络中每层的输入分布发生变化。越是深层的神经网络，其输入的分布会改变的越明显。 解决方法(归一化操作)：从机器学习角度来看，如果某层的输入分布发生了变化，那么其参数需要重新学习，这种现象称为内部协变量偏移。为了解决内部协变量偏移问题，就要使得每一层神经网络输入的分布在训练过程中要保持一致。最简单的方法是对每一层神经网络都进行归一化操作，使其分布保持稳定。 1.批量归一化 协变量偏移介绍 在传统机器学习中，一个常见的问题是协变量偏移。协变量是一个统计学概念，是可能影响预测结果的统计变量。在机器学习中，协变量可以看作是输入。一般的机器学习算法都要求输入在训练集和测试集上的分布式相似的。如果不满足这个假设，在训练集上学习到的模型在测试集上的表现会比较差，如下图所示： BN原理介绍 批量归一化方法是一种有效的逐层归一化的方法，可以对神经网络中任意的中间层进行归一化操作。对一个深层神经网络来说，令第l层的净输入为$\\mathbf{z}^{(l)}$， 经过激活函数后的输出是$\\mathbf{a}^{(l)}$，即 \\mathbf{a}^{(l)}=f\\left(\\mathbf{z}^{(l)}\\right)=f\\left(W \\mathbf{a}^{(l-1)}+\\mathbf{b}\\right)其中，$f(\\cdot)$是激活函数，W和b是权重和偏置参数。 为了减少内部协变量偏移问题，就要使得净输入$\\mathbf{z}^{(l)}$的分布一致，比如都归一化到标准正态分布。虽然归一化操作可以应用在输入$\\mathbf{a}^{(l-1)}$上，但其分布性质不如$\\mathbf{z}^{(l)}$稳定。因此，在实践中归一化操作一般应用在仿射变换之后，在下一次激活函数之前。利用数据预处理方法对$\\mathbf{z}^{(l)}$进行归一化，相当于每一层都进行一次数据预处理，从而加速收敛速度。但是，逐层归一化需要在中间层进行操作，要求效率比较高，因此复杂度比较高的白话方法就不太合适。为了提高归一化效率，一般使用标准归一化，将净输入$\\mathbf{z}^{(l)}$的每一维都归一到标准正态分布。 \\hat{\\mathbf{z}}^{(l)}=\\frac{\\mathbf{z}^{(l)}-\\mathbb{E}\\left[\\mathbf{z}^{(l)}\\right]}{\\sqrt{\\operatorname{var}\\left(\\mathbf{z}^{(l)}\\right)+\\epsilon}}其中，$\\mathbb{E}\\left[\\mathbf{z}^{(l)}\\right]$和$\\operatorname{var}\\left(\\mathbf{z}^{(l)}\\right)$是当前参数下，$\\mathbf{z}^{(l)}$的每一维度在整个训练集上的期望和方差。因为目前主要的训练方法是基于Mini-Batch的随机梯度下降算法，所以准确地计算$\\mathbf{z}^{(l)}$的期望和方差是不可行的。因此，$\\mathbf{z}^{(l)}$的期望和方差通常用当前小批量Mini-Batch样本集的均值和方差近似估计。 给定一个包含K个样本的小批量样本集合，第l层神经元的净输入$\\mathbf{z}^{(1,l)}$，….，$\\mathbf{z}^{(K,l)}$的均值和方差为： \\begin{aligned} \\mu_{\\mathcal{B}} &=\\frac{1}{K} \\sum_{k=1}^{\\mathrm{N}} \\mathbf{z}^{(k, l)} \\\\ \\sigma_{\\mathcal{B}}^{2} &=\\frac{1}{K} \\sum_{k=1}^{K}\\left(\\mathbf{z}^{(k, l)}-\\mu_{\\mathcal{B}}\\right) \\odot\\left(\\mathbf{z}^{(k, l)}-\\mu_{\\mathcal{B}}\\right) \\end{aligned} 对净输入$\\mathbf{z}^{(l)}$的标准归一化会使得其取值集中到0附近，如果使用sigmoid激活函数时，这个取值区间刚好是接近线性变换区间，从而减弱了神经网络非线性变换的性质。因此，为了使归一化操作不对网络的表示能力造成负面影响，可以通过一个附加的缩放和平移变换改变取值区间。$\\hat{\\mathbf{z}}^{(l)}=\\frac{\\mathbf{z}^{(l)}-\\mu{\\mathcal{B}}}{\\sqrt{\\sigma{\\mathcal{B}}^{2}+\\epsilon}} \\odot \\gamma+\\beta\\triangleq \\mathrm{BN}{\\gamma, \\beta}\\left(\\mathbf{z}^{(l)}\\right)$其中，$\\gamma$、$\\beta$分别表示缩放和平移的参数向量。从最保守的角度考虑，可以通过标准归一化的逆变换来使得归一化的变量可以被还原为原来的值。即：当$\\gamma=\\sqrt{\\sigma{\\mathcal{B}}^{2}}$，$\\beta=\\mu_{\\mathcal{B}}$时，$\\hat{\\mathbf{z}}^{(l)}=\\mathbf{z}^{(l)}$。 批量归一化操作可以看作是一个特殊的神经网络层，该层是加在每一层非线性激活函数之前，即： \\mathbf{a}^{(l)}=f\\left(\\mathbf{B} \\mathbf{N}_{\\gamma, \\beta}\\left(\\mathbf{z}^{(l)}\\right)\\right)=f\\left(\\mathbf{B} \\mathbf{N}_{\\gamma, \\beta}\\left(W \\mathbf{a}^{(l-1)}\\right)\\right)其中，因为批量归一化本身具有平移变换，因此非线性变换$W \\mathbf{a}^{(l-1)}$就不再需要偏置参数b。 注意：每次小批量样本的$\\mu{\\mathcal{B}}$和$\\sigma{\\mathcal{B}}^{2}$是净输入$\\mathbf{z}^{(l)}$的函数，而不是常量。因此，在计算参数梯度时，需要考虑$\\mu{\\mathcal{B}}$和$\\sigma{\\mathcal{B}}^{2}$的影响。当训练完成时，用整个数据集上的均值$\\mu$和方差$\\sigma^{2}$来分别替代每次小批量样本的$\\mu{\\mathcal{B}}$和$\\sigma{\\mathcal{B}}^{2}$。在实际中，$\\mu{\\mathcal{B}}$和$\\sigma{\\mathcal{B}}^{2}$也可以使用移动平均来计算。 2.参考资料 邱锡鹏：《神经网络与深度学习》","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://cdlwhm1217096231.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://cdlwhm1217096231.github.io/tags/深度学习/"},{"name":"人工智能","slug":"人工智能","permalink":"https://cdlwhm1217096231.github.io/tags/人工智能/"},{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://cdlwhm1217096231.github.io/tags/算法/"}]},{"title":"常见的数据预处理方法总结","slug":"常见的数据预处理方法总结","date":"2019-07-29T10:50:47.000Z","updated":"2019-07-29T11:07:49.330Z","comments":true,"path":"机器学习/常见的数据预处理方法总结/","link":"","permalink":"https://cdlwhm1217096231.github.io/机器学习/常见的数据预处理方法总结/","excerpt":"","text":"0.概述 一般而言，样本的原始特征中的每一维特征由于来源以及度量单位不同，其特征取值的分布范围往往差异很大。当我们计算不同样本之间的欧氏距离时，取值范围大的特征会起到主导作用。这样，对于基于相似度比较的机器学习方法（比如最近邻分类器KNN），必须先对样本进行预处理，将各个维度的特征归一化到同一个取值区间，并且消除不同特征之间的相关性，才能获得比较理想的结果。虽然神经网络可以通过参数的调整来适应不同特征的取值范围，但是会导致训练效率比较低。 假设一个只有一层的网络 $y=\\tanh \\left(w{1} x{1}+w{2} x{2}+b\\right)$，其中$x{1} \\in[0,10]$，$x{2} \\in[0,1]$。因为tanh函数的导数在区间 [−2, 2]上是敏感的，其余地方的导数接近于 0。因此，如果 $w{1} x{1}+w{2} x{2}+b$过大或过小，都会导致梯度过小，难以训练。为了提高训练效率，我们需要使$w{1} x{1}+w{2} x{2}+b$在 [−2, 2]区间，我们需要将w1设得小一点，比如在 [−0.1,0.1]之间。可以想象，如果数据维数很多时，我们很难这样精心去选择每一个参数。因此，如果每一个特征的取值范围都在相似的区间，比如 [0, 1]或者 [−1, 1]，我们就不太需要区别对待每一个参数，减少人工干预。 当不同输入特征的取值范围差异比较大时，梯度下降法的效率也会受到影响。下图给出了数据归一化对梯度的影响。其中，图a为未归一化数据的等高线图。取值范围不同会造成在大多数位置上的梯度方向并不是最优的搜索方向。当使用梯度下降法寻求最优解时，会导致需要很多次迭代才能收敛。如果我们把数据归一化为取值范围相同，如图b所示，大部分位置的梯度方向近似于最优搜索方向。这样，在梯度下降求解时，每一步梯度的方向都基本指向最小值，训练效率会大大提高。 1.常用的归一化方法 1.1 缩放归一化：通过缩放将每一个特征的取值范围归一到 [0, 1]或 [−1, 1]之间。假设有 N 个样本$\\left{\\mathbf{x}^{(n)}\\right}_{n=1}^{N}$，对每一维特征x， \\hat{x}^{(n)}=\\frac{x^{(n)}-\\min _{n}\\left(x^{(n)}\\right)}{\\max _{n}\\left(x^{(n)}\\right)-\\min _{n}\\left(x^{(n)}\\right)}其中，min(x)和max(x)分别是特征x在所有样本上的最小值和最大值。 1.2 标准归一化：将每一个维特征都处理为符合标准正态分布（均值为 0，标准差为 1）。假设有 N 个样本$\\left{\\mathbf{x}^{(n)}\\right}_{n=1}^{N}$，对每一维特征x，先计算它的均值和标准差： \\begin{aligned} \\mu &=\\frac{1}{N} \\sum_{n=1}^{N} x^{(n)} \\\\ \\sigma^{2} &=\\frac{1}{N} \\sum_{n=1}^{N}\\left(x^{(n)}-\\mu\\right)^{2} \\end{aligned}然后，将特征$x^{(n)}$减去均值，并除以标准差，得到新的特征$\\hat{x}^{(n)}$。 \\hat{x}^{(n)}=\\frac{x^{(n)}-\\mu}{\\sigma}这里$\\sigma$不能为0，如果标准差为0，则说明这一维度的特征没有任务的区分性，可以直接删除。在标准归一化之后，每一维特征都服从标准正态分布。 1.3 白化：是一种重要的预处理方法，用来降低输入数据特征之间的冗余性。输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差。白化的一个主要实现方式是使用主成分分析PCA方法去除掉各个成分之间的相关性。 2. 参考资料 邱锡鹏：《神经网络与深度学习》","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/categories/机器学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://cdlwhm1217096231.github.io/tags/人工智能/"},{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://cdlwhm1217096231.github.io/tags/算法/"}]},{"title":"复习02统计学习方法(感知机perceptron machine)---图片版","slug":"复习02统计学习方法-感知机perceptron-machine-图片版","date":"2019-07-29T10:48:41.000Z","updated":"2019-07-29T11:07:36.241Z","comments":true,"path":"机器学习/复习02统计学习方法-感知机perceptron-machine-图片版/","link":"","permalink":"https://cdlwhm1217096231.github.io/机器学习/复习02统计学习方法-感知机perceptron-machine-图片版/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/categories/机器学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://cdlwhm1217096231.github.io/tags/人工智能/"},{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://cdlwhm1217096231.github.io/tags/算法/"}]},{"title":"复习01统计学习方法概论(机器学习中的重要概念)---图片版","slug":"复习01统计学习方法概论-机器学习中的重要概念-图片版","date":"2019-07-29T10:46:17.000Z","updated":"2019-07-29T11:08:01.053Z","comments":true,"path":"机器学习/复习01统计学习方法概论-机器学习中的重要概念-图片版/","link":"","permalink":"https://cdlwhm1217096231.github.io/机器学习/复习01统计学习方法概论-机器学习中的重要概念-图片版/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/categories/机器学习/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://cdlwhm1217096231.github.io/tags/人工智能/"},{"name":"机器学习","slug":"机器学习","permalink":"https://cdlwhm1217096231.github.io/tags/机器学习/"},{"name":"算法","slug":"算法","permalink":"https://cdlwhm1217096231.github.io/tags/算法/"}]},{"title":"数据库中的基本概念","slug":"数据库中的基本概念","date":"2019-07-28T06:50:37.000Z","updated":"2019-07-28T04:05:54.000Z","comments":true,"path":"数据库/数据库中的基本概念/","link":"","permalink":"https://cdlwhm1217096231.github.io/数据库/数据库中的基本概念/","excerpt":"","text":"1.基本概念 数据（data）：描述事物的符号记录称为数据 数据库（DataBase，DB）：是长期存储在计算机内、有组织的、可共享的大量数据的集合，具有永久存储、有组织、可共享三个基本特点。 数据库管理系统（DataBase Management System，DBMS）：是位于用户与操作系统之间的一层数据管理软件。 数据库系统（DataBase System，DBS）：是有数据库、数据库管理系统（及其应用开发工具）、应用程序和数据库管理员（DataBase Administrator DBA）组成的存储、管理、处理和维护数据的系统。 实体（entity）：客观存在并可相互区别的事物称为实体。 属性（attribute）：实体所具有的某一特性称为属性。 码（key）：唯一标识实体的属性集称为码。 实体型（entity type）：用实体名及其属性名集合来抽象和刻画同类实体，称为实体型。 实体集（entity set）：同一实体型的集合称为实体集。 联系（relationship）：实体之间的联系通常是指不同实体集之间的联系。 模式（schema）：模式也称逻辑模式，是数据库全体数据的逻辑结构和特征的描述，是所有用户的公共数据视图。 外模式（external schema）：外模式也称子模式（subschema）或用户模式，它是数据库用户（包括应用程序员和最终用户）能够看见和使用的局部数据的逻辑结构和特征的描述，是数据库用户的数据视图，是与某一应用有关的数据的逻辑表示。 内模式（internal schema）：内模式也称为存储模式（storage schema），一个数据库只有一个内模式。他是数据物理结构和存储方式的描述，是数据库在数据库内部的组织方式。2.常用数据模型 层次模型（hierarchical model） 网状模型（network model） 关系模型（relational model） 关系（relation）：一个关系对应通常说的一张表 元组（tuple）：表中的一行即为一个元组 属性（attribute）：表中的一列即为一个属性 码（key）：表中可以唯一确定一个元组的某个属性组 域（domain）：一组具有相同数据类型的值的集合 分量：元组中的一个属性值 关系模式：对关系的描述，一般表示为 关系名(属性1, 属性2, …, 属性n) 面向对象数据模型（object oriented data model） 对象关系数据模型（object relational data model） 半结构化数据模型（semistructure data model）3.常用SQL操作 更多操作请参见我之前总结的数据库基本操作4.关系型数据库 基本关系操作：查询（选择、投影、连接（等值连接、自然连接、外连接（左外连接、右外连接））、除、并、差、交、笛卡尔积等）、插入、删除、修改 关系模型中的三类完整性约束：实体完整性、参照完整性、用户定义的完整性5.索引 数据库索引：顺序索引、B+ 树索引、hash 索引 更多信息: MySQL 索引背后的数据结构及算法原理6.数据库完整性 数据库的完整性是指数据的正确性和相容性。 完整性：为了防止数据库中存在不符合语义（不正确）的数据。 安全性：为了保护数据库防止恶意破坏和非法存取。 触发器：是用户定义在关系表中的一类由事件驱动的特殊过程。7.关系数据理论 数据依赖是一个关系内部属性与属性之间的一种约束关系，是通过属性间值的相等与否体现出来的数据间相关联系。 最重要的数据依赖：函数依赖、多值依赖。8.范式 第一范式（1NF）：属性（字段）是最小单位不可再分。 第二范式（2NF）：满足 1NF，每个非主属性完全依赖于主键（消除 1NF 非主属性对码的部分函数依赖）。 第三范式（3NF）：满足 2NF，任何非主属性不依赖于其他非主属性（消除 2NF 主属性对码的传递函数依赖）。 鲍依斯-科得范式（BCNF）：满足 3NF，任何非主属性不能对主键子集依赖（消除 3NF 主属性对码的部分和传递函数依赖）。 第四范式（4NF）：满足 3NF，属性之间不能有非平凡且非函数依赖的多值依赖（消除 3NF 非平凡且非函数依赖的多值依赖）。9.数据库恢复 事务：是用户定义的一个数据库操作序列，这些操作要么全做，要么全不做，是一个不可分割的工作单位。 事物的 ACID 特性：原子性、一致性、隔离性、持续性。 恢复的实现技术：建立冗余数据 -&gt; 利用冗余数据实施数据库恢复。 建立冗余数据常用技术：数据转储（动态海量转储、动态增量转储、静态海量转储、静态增量转储）、登记日志文件。10.并发控制 事务是并发控制的基本单位。 并发操作带来的数据不一致性包括：丢失修改、不可重复读、读 “脏” 数据。 并发控制主要技术：封锁、时间戳、乐观控制法、多版本并发控制等。 基本封锁类型：排他锁（X 锁 / 写锁）、共享锁（S 锁 / 读锁）。 活锁和死锁： 活锁：事务永远处于等待状态，可通过先来先服务的策略避免。 死锁：事物永远不能结束 预防：一次封锁法、顺序封锁法； 诊断：超时法、等待图法； 解除：撤销处理死锁代价最小的事务，并释放此事务的所有的锁，使其他事务得以继续运行下去。 可串行化调度：多个事务的并发执行是正确的，当且仅当其结果与按某一次序串行地执行这些事务时的结果相同。可串行性时并发事务正确调度的准则。参考博客 数据库简明教程","categories":[{"name":"数据库","slug":"数据库","permalink":"https://cdlwhm1217096231.github.io/categories/数据库/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://cdlwhm1217096231.github.io/tags/数据库/"}]}]}